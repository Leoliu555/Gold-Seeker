"""
Gold-Seeker: AI Mineral Prediction System
Streamlit Frontend Application

åŸºäºStreamlitçš„åœ°çƒåŒ–å­¦æ‰¾çŸ¿é¢„æµ‹äº¤äº’å¼ç•Œé¢
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Circle
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import folium
from streamlit_folium import st_folium
import json
import io
import base64
from pathlib import Path
import sys
import warnings
import requests
import time
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.spatial.distance import pdist

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

# ç›´æ¥å¯¼å…¥åç«¯å¤„ç†å™¨ï¼Œé¿å…å¯¼å…¥æ•´ä¸ªagentsåŒ…
sys.path.append(str(Path(__file__).parent / 'agents' / 'tools' / 'geochem'))
from processor import GeochemProcessor

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'FangSong', 'SimSun']
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
plt.rcParams['font.size'] = 10
plt.rcParams['axes.titlesize'] = 14
plt.rcParams['axes.labelsize'] = 12
plt.rcParams['xtick.labelsize'] = 10
plt.rcParams['ytick.labelsize'] = 10

# è®¾ç½®seabornä¸­æ–‡å­—ä½“
sns.set_style('whitegrid')
sns.set_palette('husl')

# è®¾ç½®Plotlyä¸­æ–‡å­—ä½“
import plotly.io as pio
pio.templates.default = "plotly_white"
# è®¾ç½®ä¸­æ–‡å­—ä½“
font_config = {
    'family': 'Microsoft YaHei, SimHei, FangSong, SimSun, Arial',
    'size': 12,
    'color': '#333333'
}
pio.templates["custom"] = {
    'layout': {
        'font': font_config,
        'title': {
            'font': {
                'family': '"Microsoft YaHei", "SimHei", "Arial", sans-serif',
                'size': 16
            }
        },
        'xaxis': {
            'title': {
                'font': {
                    'family': '"Microsoft YaHei", "SimHei", "Arial", sans-serif',
                    'size': 14
                }
            },
            'tickfont': {
                'family': '"Microsoft YaHei", "SimHei", "Arial", sans-serif',
                'size': 12
            }
        },
        'yaxis': {
            'title': {
                'font': {
                    'family': '"Microsoft YaHei", "SimHei", "Arial", sans-serif',
                    'size': 14
                }
            },
            'tickfont': {
                'family': '"Microsoft YaHei", "SimHei", "Arial", sans-serif',
                'size': 12
            }
        }
    }
}

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="Gold-Seeker: AI Mineral Prediction System",
    page_icon="â›ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSSæ ·å¼
def set_custom_style():
    """è®¾ç½®è‡ªå®šä¹‰æ ·å¼"""
    st.markdown("""
    <style>
    .main {
        background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
        color: white;
    }
    .stSidebar {
        background: linear-gradient(135deg, #2c3e50 0%, #34495e 100%);
        color: white;
    }
    .stTabs [data-baseweb="tab-list"] {
        background-color: #2c3e50;
    }
    .stTabs [data-baseweb="tab"] {
        background-color: #34495e;
        color: white;
    }
    .plot-container {
        background-color: rgba(255, 255, 255, 0.1);
        border-radius: 10px;
        padding: 20px;
        margin: 10px 0;
    }
    .chat-message {
        background-color: rgba(255, 255, 255, 0.1);
        border-radius: 10px;
        padding: 15px;
        margin: 10px 0;
    }
    .agent-message {
        background-color: rgba(52, 152, 219, 0.2);
        border-left: 4px solid #3498db;
    }
    .user-message {
        background-color: rgba(46, 204, 113, 0.2);
        border-left: 4px solid #2ecc71;
    }
    </style>
    """, unsafe_allow_html=True)

# åˆå§‹åŒ–session state
def init_session_state():
    """åˆå§‹åŒ–session state"""
    if 'data' not in st.session_state:
        st.session_state.data = None
    if 'agent' not in st.session_state:
        st.session_state.agent = None
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    if 'analysis_results' not in st.session_state:
        st.session_state.analysis_results = {}
    if 'selected_elements' not in st.session_state:
        st.session_state.selected_elements = ['Au', 'As', 'Sb', 'Hg']
    if 'target_mineral' not in st.session_state:
        st.session_state.target_mineral = 'Au'
    if 'deepseek_api_key' not in st.session_state:
        st.session_state.deepseek_api_key = 'sk-5bb78328de57481ea1f463325f209b02'

# ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
def generate_mock_data(n_samples=200):
    """ç”Ÿæˆæ¨¡æ‹Ÿåœ°çƒåŒ–å­¦æ•°æ® - ä½¿ç”¨æ­£ç¡®çš„åœ°ç†åæ ‡"""
    np.random.seed(42)
    
    # ç”Ÿæˆåˆç†çš„åœ°ç†åæ ‡èŒƒå›´ (ç»åº¦: 100-101Â°, çº¬åº¦: 30-31Â°)
    data = {
        'X': np.random.uniform(100.0, 101.0, n_samples),  # ç»åº¦èŒƒå›´
        'Y': np.random.uniform(30.0, 31.0, n_samples),    # çº¬åº¦èŒƒå›´
        'Au': np.random.lognormal(0, 1, n_samples),
        'As': np.random.lognormal(1, 0.8, n_samples),
        'Sb': np.random.lognormal(0.5, 0.9, n_samples),
        'Hg': np.random.lognormal(-0.5, 1.2, n_samples),
        'Cu': np.random.lognormal(2, 0.7, n_samples),
        'Pb': np.random.lognormal(1.5, 0.6, n_samples),
        'Zn': np.random.lognormal(2.2, 0.5, n_samples),
        'Ag': np.random.lognormal(-0.2, 1.0, n_samples),
    }
    
    # æ·»åŠ ä¸€äº›ä½äºæ£€æµ‹é™çš„å€¼
    detection_limits = {'Au': 0.05, 'As': 0.5, 'Sb': 0.2, 'Hg': 0.01}
    for element, limit in detection_limits.items():
        censored_mask = np.random.random(n_samples) < 0.2
        data[element][censored_mask] = np.random.uniform(0, limit, censored_mask.sum())
    
    # æ·»åŠ è®­ç»ƒç‚¹æ ‡ç­¾
    data['Is_Deposit'] = np.zeros(n_samples, dtype=int)
    deposit_indices = np.random.choice(n_samples, size=20, replace=False)
    for idx in deposit_indices:
        data['Is_Deposit'][idx] = 1
        data['Au'][idx] *= np.random.uniform(5, 20)
        data['As'][idx] *= np.random.uniform(3, 10)
        data['Sb'][idx] *= np.random.uniform(2, 8)
    
    return pd.DataFrame(data)

# ç”Ÿæˆç›¸å…³æ€§çƒ­åŠ›å›¾ (è°ƒç”¨åç«¯)
def create_correlation_heatmap(data, elements):
    """åˆ›å»ºç›¸å…³æ€§çƒ­åŠ›å›¾"""
    processor = GeochemProcessor()
    return processor.plot_correlation_heatmap(data, elements)

# ç”ŸæˆRå‹èšç±»æ ‘çŠ¶å›¾ (è°ƒç”¨åç«¯)
def create_dendrogram(data, elements):
    """åˆ›å»ºRå‹èšç±»æ ‘çŠ¶å›¾"""
    processor = GeochemProcessor()
    return processor.plot_dendrogram(data, elements)

# ç”ŸæˆPCAè½½è·å›¾ (è°ƒç”¨åç«¯)
def create_pca_loadings_plot(data, elements):
    """åˆ›å»ºPCAè½½è·å›¾"""
    processor = GeochemProcessor()
    return processor.plot_pca_loadings(data, elements)

# ç”ŸæˆC-Aåˆ†å½¢å›¾
def create_ca_fractal_plot(data, element):
    """åˆ›å»ºC-Aåˆ†å½¢å›¾"""
    # æ¨¡æ‹ŸC-Aåˆ†å½¢åˆ†æ
    concentrations = np.sort(data[element].values)
    areas = np.arange(1, len(concentrations) + 1)
    
    # å¯¹æ•°å˜æ¢
    log_conc = np.log10(concentrations[concentrations > 0])
    log_area = np.log10(areas[concentrations > 0])
    
    # æ¨¡æ‹Ÿæ‹ç‚¹
    threshold_idx = int(len(log_conc) * 0.8)
    threshold = concentrations[threshold_idx]
    
    fig, ax = plt.subplots(figsize=(10, 8))
    
    # ç»˜åˆ¶æ•£ç‚¹å›¾
    ax.scatter(log_conc, log_area, alpha=0.6, s=30, c='blue', label='æ•°æ®ç‚¹')
    
    # æ‹ŸåˆèƒŒæ™¯çº¿
    bg_mask = log_conc < np.log10(threshold)
    if bg_mask.sum() > 1:
        bg_fit = np.polyfit(log_conc[bg_mask], log_area[bg_mask], 1)
        bg_line = np.poly1d(bg_fit)
        ax.plot(log_conc[bg_mask], bg_line(log_conc[bg_mask]), 
                'r--', linewidth=2, label='èƒŒæ™¯æ‹Ÿåˆ')
    
    # æ‹Ÿåˆå¼‚å¸¸çº¿
    anom_mask = log_conc >= np.log10(threshold)
    if anom_mask.sum() > 1:
        anom_fit = np.polyfit(log_conc[anom_mask], log_area[anom_mask], 1)
        anom_line = np.poly1d(anom_fit)
        ax.plot(log_conc[anom_mask], anom_line(log_conc[anom_mask]), 
                'g--', linewidth=2, label='å¼‚å¸¸æ‹Ÿåˆ')
    
    # æ ‡è®°æ‹ç‚¹
    ax.axvline(x=np.log10(threshold), color='red', linestyle=':', 
               linewidth=2, label=f'é˜ˆå€¼: {threshold:.3f}')
    
    ax.set_xlabel('log(æµ“åº¦)', fontsize=12)
    ax.set_ylabel('log(é¢ç§¯)', fontsize=12)
    ax.set_title(f'{element} C-Aåˆ†å½¢åˆ†æ', fontsize=16, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    
    return fig, threshold

# åˆ›å»ºäº¤äº’å¼åœ°å›¾
def create_comprehensive_analysis_panel(data, element):
    """åˆ›å»ºç»¼åˆåˆ†æé¢æ¿ - ç±»ä¼¼demo_comprehensive_analysis.pngçš„ä¸“ä¸šå±•ç¤º"""
    processor = GeochemProcessor()
    
    # åˆ›å»º4ä¸ªå­å›¾çš„ç»¼åˆåˆ†æ
    fig = plt.figure(figsize=(16, 12))
    
    # 1. åŸå§‹æ•°æ®åˆ†å¸ƒå›¾ (å·¦ä¸Š)
    ax1 = plt.subplot(2, 3, 1)
    scatter = ax1.scatter(data['X'], data['Y'], c=data[element], 
                         cmap='YlOrRd', s=50, alpha=0.7, edgecolors='black', linewidth=0.5)
    ax1.set_xlabel('Longitude (Â°)', fontsize=10)
    ax1.set_ylabel('Latitude (Â°)', fontsize=10)
    ax1.set_title(f'(a) {element} Distribution', fontsize=12, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    plt.colorbar(scatter, ax=ax1, label=f'{element} (ppm)')
    
    # 2. ç›´æ–¹å›¾å’Œç»Ÿè®¡ (å³ä¸Š)
    ax2 = plt.subplot(2, 3, 2)
    element_data = data[element].dropna()
    ax2.hist(element_data, bins=30, color='skyblue', alpha=0.7, edgecolor='black')
    ax2.set_xlabel(f'{element} Concentration (ppm)', fontsize=10)
    ax2.set_ylabel('Frequency', fontsize=10)
    ax2.set_title(f'(b) {element} Histogram', fontsize=12, fontweight='bold')
    ax2.grid(True, alpha=0.3)
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    mean_val = element_data.mean()
    std_val = element_data.std()
    median_val = element_data.median()
    ax2.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}')
    ax2.axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Median: {median_val:.2f}')
    ax2.legend(fontsize=8)
    
    # 3. QQå›¾ (å·¦ä¸­)
    ax3 = plt.subplot(2, 3, 3)
    from scipy import stats
    stats.probplot(element_data, dist="norm", plot=ax3)
    ax3.set_xlabel('Theoretical Quantiles', fontsize=10)
    ax3.set_ylabel('Sample Quantiles', fontsize=10)
    ax3.set_title(f'(c) {element} Q-Q Plot', fontsize=12, fontweight='bold')
    ax3.grid(True, alpha=0.3)
    
    # 4. ç®±çº¿å›¾ (å³ä¸­)
    ax4 = plt.subplot(2, 3, 4)
    box_plot = ax4.boxplot(element_data, patch_artist=True)
    box_plot['boxes'][0].set_facecolor('lightblue')
    ax4.set_ylabel(f'{element} Concentration (ppm)', fontsize=10)
    ax4.set_title(f'(d) {element} Box Plot', fontsize=12, fontweight='bold')
    ax4.grid(True, alpha=0.3)
    
    # 5. ç´¯ç§¯åˆ†å¸ƒå‡½æ•° (å·¦ä¸‹)
    ax5 = plt.subplot(2, 3, 5)
    sorted_data = np.sort(element_data)
    cumulative = np.arange(1, len(sorted_data) + 1) / len(sorted_data)
    ax5.plot(sorted_data, cumulative, linewidth=2, color='blue')
    ax5.set_xlabel(f'{element} Concentration (ppm)', fontsize=10)
    ax5.set_ylabel('Cumulative Probability', fontsize=10)
    ax5.set_title(f'(e) {element} CDF', fontsize=12, fontweight='bold')
    ax5.grid(True, alpha=0.3)
    
    # 6. ç»Ÿè®¡æ‘˜è¦è¡¨ (å³ä¸‹)
    ax6 = plt.subplot(2, 3, 6)
    ax6.axis('off')
    
    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    stats_summary = {
        'Count': len(element_data),
        'Mean': f'{mean_val:.3f}',
        'Std Dev': f'{std_val:.3f}',
        'Min': f'{element_data.min():.3f}',
        'Max': f'{element_data.max():.3f}',
        'Median': f'{median_val:.3f}',
        'Skewness': f'{stats.skew(element_data):.3f}',
        'Kurtosis': f'{stats.kurtosis(element_data):.3f}'
    }
    
    # åˆ›å»ºç»Ÿè®¡è¡¨æ ¼
    table_data = []
    for key, value in stats_summary.items():
        table_data.append([key, value])
    
    table = ax6.table(cellText=table_data, 
                     colLabels=['Statistic', 'Value'],
                     cellLoc='center',
                     loc='center',
                     bbox=[0.1, 0.1, 0.8, 0.8])
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 1.5)
    
    # è®¾ç½®è¡¨æ ¼æ ·å¼
    for i in range(len(table_data) + 1):
        for j in range(2):
            cell = table[i, j]
            if i == 0:  # è¡¨å¤´
                cell.set_facecolor('#4CAF50')
                cell.set_text_props(weight='bold', color='white')
            else:
                cell.set_facecolor('#f0f0f0' if i % 2 == 0 else 'white')
    
    ax6.set_title(f'(f) {element} Statistics Summary', fontsize=12, fontweight='bold', pad=20)
    
    plt.tight_layout()
    plt.subplots_adjust(top=0.93)
    
    # æ·»åŠ æ€»æ ‡é¢˜
    fig.suptitle(f'Comprehensive Geochemical Analysis - {element}', 
                 fontsize=16, fontweight='bold', y=0.98)
    
    st.pyplot(fig)
    plt.close(fig)
    
    # æ˜¾ç¤ºå…³é”®ç»Ÿè®¡æŒ‡æ ‡
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("æ ·å“æ•°é‡", len(element_data))
    with col2:
        st.metric("å¹³å‡å€¼", f"{mean_val:.3f} ppm")
    with col3:
        st.metric("æ ‡å‡†å·®", f"{std_val:.3f} ppm")
    with col4:
        st.metric("å˜å¼‚ç³»æ•°", f"{std_val/mean_val*100:.1f}%")
    
    # æ·»åŠ AIè§£é‡ŠæŒ‰é’®
    if st.button("ğŸ¤– AIè§£é‡Šç»¼åˆåˆ†æ", key="explain_comprehensive"):
        with st.spinner("ğŸ¤– AIæ­£åœ¨ç”Ÿæˆè§£é‡Š..."):
            from scipy import stats
            
            explanation_prompt = f"""
è¯·è§£é‡Šä»¥ä¸‹åœ°çƒåŒ–å­¦å…ƒç´ ç»¼åˆåˆ†æç»“æœï¼š

## ç»¼åˆç»Ÿè®¡åˆ†æç»“æœ
- åˆ†æå…ƒç´ : {element}
- æ ·æœ¬æ•°é‡: {len(element_data)}
- å¹³å‡å€¼: {mean_val:.3f} ppm
- æ ‡å‡†å·®: {std_val:.3f} ppm
- ä¸­ä½æ•°: {median_val:.3f} ppm
- æœ€å°å€¼: {element_data.min():.3f} ppm
- æœ€å¤§å€¼: {element_data.max():.3f} ppm
- ååº¦: {stats.skew(element_data):.3f}
- å³°åº¦: {stats.kurtosis(element_data):.3f}
- å˜å¼‚ç³»æ•°: {std_val/mean_val*100:.1f}%

## åˆ†å¸ƒç‰¹å¾
- æ•°æ®åˆ†å¸ƒ: {'æ­£æ€åˆ†å¸ƒ' if abs(stats.skew(element_data)) < 0.5 else 'åæ€åˆ†å¸ƒ'}
- å¼‚å¸¸å€¼æƒ…å†µ: {'å­˜åœ¨å¼‚å¸¸å€¼' if abs(stats.kurtosis(element_data)) > 3 else 'æ— æ˜æ˜¾å¼‚å¸¸å€¼'}
- æ•°æ®ç¦»æ•£ç¨‹åº¦: {'é«˜' if std_val/mean_val > 0.5 else 'ä¸­' if std_val/mean_val > 0.2 else 'ä½'}

è¯·ä»åœ°è´¨å­¦è§’åº¦è§£é‡Šï¼š
1. å…ƒç´ åˆ†å¸ƒçš„ç»Ÿè®¡ç‰¹å¾å’Œåœ°è´¨æ„ä¹‰
2. ååº¦å’Œå³°åº¦å¯¹æˆçŸ¿ä½œç”¨çš„æŒ‡ç¤º
3. æ•°æ®ç¦»æ•£ç¨‹åº¦ä¸åœ°è´¨è¿‡ç¨‹çš„å…³ç³»
4. å¯¹é‡‘çŸ¿å‹˜æ¢çš„æŒ‡å¯¼æ„ä¹‰
5. ä¸‹ä¸€æ­¥å·¥ä½œå»ºè®®

è¯·ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€è§£é‡Šï¼Œä¾¿äºåœ°è´¨å‹˜æ¢äººå‘˜ç†è§£ã€‚
"""
            
            api_key = st.session_state.get('deepseek_api_key', '')
            explanation = call_deepseek_api(explanation_prompt, api_key)
            
            if not explanation.startswith("âŒ"):
                st.markdown("**ğŸ§  AIåœ°è´¨è§£é‡Šï¼š**")
                st.markdown(explanation)
            else:
                st.error(explanation)

def create_professional_kriging_display(data, element, kriging_result, threshold=None):
    """åˆ›å»ºä¸“ä¸šçš„å…‹é‡Œé‡‘æ’å€¼å±•ç¤º - ç±»ä¼¼demo_kriging_result.png"""
    
    # åˆ›å»º4ä¸ªå­å›¾çš„ä¸“ä¸šå±•ç¤º
    fig = plt.figure(figsize=(16, 12))
    
    # 1. å…‹é‡Œé‡‘æ’å€¼çƒ­åŠ›å›¾ (å·¦ä¸Š)
    ax1 = plt.subplot(2, 3, 1)
    im1 = ax1.imshow(kriging_result['grid_z'].T, 
                      extent=kriging_result['extent'],
                      origin='lower', 
                      cmap='YlOrRd',
                      alpha=0.9)
    
    # æ·»åŠ åŸå§‹æ•°æ®ç‚¹
    ax1.scatter(data['X'], data['Y'], c='black', s=30, alpha=0.7, 
               edgecolors='white', linewidth=0.5, zorder=5)
    
    ax1.set_xlabel('Longitude (Â°)', fontsize=10)
    ax1.set_ylabel('Latitude (Â°)', fontsize=10)
    ax1.set_title(f'(a) {element} Kriging Interpolation', fontsize=12, fontweight='bold')
    plt.colorbar(im1, ax=ax1, label=f'{element} (ppm)')
    ax1.grid(True, alpha=0.3)
    
    # 2. ç­‰å€¼çº¿å›¾ (å³ä¸Š)
    ax2 = plt.subplot(2, 3, 2)
    contour = ax2.contour(kriging_result['grid_x'], kriging_result['grid_y'], 
                         kriging_result['grid_z'].T, levels=15, colors='black', linewidths=0.8)
    ax2.clabel(contour, inline=True, fontsize=8, fmt='%.2f')
    
    # æ·»åŠ å¡«å……ç­‰å€¼çº¿
    contourf = ax2.contourf(kriging_result['grid_x'], kriging_result['grid_y'], 
                           kriging_result['grid_z'].T, levels=15, cmap='YlOrRd', alpha=0.7)
    
    # æ·»åŠ åŸå§‹æ•°æ®ç‚¹
    ax2.scatter(data['X'], data['Y'], c='blue', s=30, alpha=0.7, 
               edgecolors='white', linewidth=0.5, zorder=5)
    
    ax2.set_xlabel('Longitude (Â°)', fontsize=10)
    ax2.set_ylabel('Latitude (Â°)', fontsize=10)
    ax2.set_title(f'(b) {element} Contour Map', fontsize=12, fontweight='bold')
    plt.colorbar(contourf, ax=ax2, label=f'{element} (ppm)')
    ax2.grid(True, alpha=0.3)
    
    # 3. å˜å·®å‡½æ•°å›¾ (å·¦ä¸­)
    ax3 = plt.subplot(2, 3, 3)
    
    # æ¨¡æ‹Ÿå˜å·®å‡½æ•°æ•°æ®
    if 'variogram_params' in kriging_result:
        variogram_params = kriging_result['variogram_params']
        nugget = variogram_params.get('nugget', 0.1)
        sill = variogram_params.get('sill', 1.0)
        range_val = variogram_params.get('range', 0.5)
    else:
        nugget, sill, range_val = 0.1, 1.0, 0.5
    
    # ç”Ÿæˆç†è®ºå˜å·®å‡½æ•°
    distances = np.linspace(0, range_val * 2, 100)
    theoretical_variogram = sill * (1 - np.exp(-3 * distances / range_val)) + nugget
    
    ax3.plot(distances, theoretical_variogram, 'b-', linewidth=2, label='Theoretical Variogram')
    ax3.axhline(y=sill, color='r', linestyle='--', alpha=0.7, label=f'Sill: {sill:.3f}')
    ax3.axhline(y=nugget, color='g', linestyle='--', alpha=0.7, label=f'Nugget: {nugget:.3f}')
    ax3.axvline(x=range_val, color='orange', linestyle='--', alpha=0.7, label=f'Range: {range_val:.3f}')
    
    ax3.set_xlabel('Distance (Â°)', fontsize=10)
    ax3.set_ylabel('Variogram', fontsize=10)
    ax3.set_title('(c) Variogram Model', fontsize=12, fontweight='bold')
    ax3.legend(fontsize=8)
    ax3.grid(True, alpha=0.3)
    ax3.set_xlim(0, range_val * 2)
    
    # 4. äº¤å‰éªŒè¯å›¾ (å³ä¸­)
    ax4 = plt.subplot(2, 3, 4)
    
    # æ¨¡æ‹Ÿäº¤å‰éªŒè¯æ•°æ®
    actual_values = data[element].values
    predicted_values = []
    
    # å¯¹æ¯ä¸ªæ•°æ®ç‚¹è¿›è¡Œæ’å€¼é¢„æµ‹
    for idx, row in data.iterrows():
        # ç®€å•çš„æœ€è¿‘é‚»æ’å€¼ä½œä¸ºé¢„æµ‹
        distances = np.sqrt((data['X'] - row['X'])**2 + (data['Y'] - row['Y'])**2)
        nearest_idx = distances[distances > 0].idxmin()
        predicted_values.append(data.loc[nearest_idx, element])
    
    predicted_values = np.array(predicted_values)
    
    # ç»˜åˆ¶æ•£ç‚¹å›¾
    ax4.scatter(actual_values, predicted_values, alpha=0.6, s=30)
    
    # æ·»åŠ 1:1çº¿
    min_val = min(actual_values.min(), predicted_values.min())
    max_val = max(actual_values.max(), predicted_values.max())
    ax4.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='1:1 Line')
    
    # è®¡ç®—RÂ²
    correlation = np.corrcoef(actual_values, predicted_values)[0, 1]
    r_squared = correlation ** 2
    
    ax4.set_xlabel('Observed Values (ppm)', fontsize=10)
    ax4.set_ylabel('Predicted Values (ppm)', fontsize=10)
    ax4.set_title(f'(d) Cross-Validation (RÂ² = {r_squared:.3f})', fontsize=12, fontweight='bold')
    ax4.legend(fontsize=8)
    ax4.grid(True, alpha=0.3)
    
    # 5. æ’å€¼è¯¯å·®åˆ†å¸ƒ (å·¦ä¸‹)
    ax5 = plt.subplot(2, 3, 5)
    
    # è®¡ç®—æ’å€¼è¯¯å·®
    residuals = actual_values - predicted_values
    
    ax5.hist(residuals, bins=20, color='skyblue', alpha=0.7, edgecolor='black')
    ax5.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero Error')
    ax5.set_xlabel('Residuals (ppm)', fontsize=10)
    ax5.set_ylabel('Frequency', fontsize=10)
    ax5.set_title('(e) Residual Distribution', fontsize=12, fontweight='bold')
    ax5.legend(fontsize=8)
    ax5.grid(True, alpha=0.3)
    
    # 6. æ’å€¼ç»Ÿè®¡è¡¨ (å³ä¸‹)
    ax6 = plt.subplot(2, 3, 6)
    ax6.axis('off')
    
    # è®¡ç®—æ’å€¼ç»Ÿè®¡
    interpolation_stats = {
        'Grid Size': f"{len(kriging_result['grid_x'])}Ã—{len(kriging_result['grid_y'])}",
        'Data Points': str(len(data)),
        'Min Value': f"{kriging_result['grid_z'].min():.3f}",
        'Max Value': f"{kriging_result['grid_z'].max():.3f}",
        'Mean Value': f"{kriging_result['grid_z'].mean():.3f}",
        'Std Dev': f"{kriging_result['grid_z'].std():.3f}",
        'Nugget': f"{nugget:.3f}",
        'Sill': f"{sill:.3f}",
        'Range': f"{range_val:.3f}",
        'RÂ²': f"{r_squared:.3f}",
        'RMSE': f"{np.sqrt(np.mean(residuals**2)):.3f}"
    }
    
    # åˆ›å»ºç»Ÿè®¡è¡¨æ ¼
    table_data = []
    for key, value in interpolation_stats.items():
        table_data.append([key, value])
    
    table = ax6.table(cellText=table_data, 
                     colLabels=['Parameter', 'Value'],
                     cellLoc='center',
                     loc='center',
                     bbox=[0.1, 0.1, 0.8, 0.8])
    table.auto_set_font_size(False)
    table.set_fontsize(9)
    table.scale(1, 1.3)
    
    # è®¾ç½®è¡¨æ ¼æ ·å¼
    for i in range(len(table_data) + 1):
        for j in range(2):
            cell = table[i, j]
            if i == 0:  # è¡¨å¤´
                cell.set_facecolor('#2196F3')
                cell.set_text_props(weight='bold', color='white')
            else:
                cell.set_facecolor('#f0f0f0' if i % 2 == 0 else 'white')
    
    ax6.set_title('(f) Interpolation Statistics', fontsize=12, fontweight='bold', pad=20)
    
    plt.tight_layout()
    plt.subplots_adjust(top=0.93)
    
    # æ·»åŠ æ€»æ ‡é¢˜
    fig.suptitle(f'Professional Kriging Analysis - {element}', 
                 fontsize=16, fontweight='bold', y=0.98)
    
    st.pyplot(fig)
    plt.close(fig)

def create_heatmap_display(data, element, threshold=None, kriging_result=None):
    """åˆ›å»ºç®€åŒ–çš„çƒ­åŠ›å›¾æ˜¾ç¤º"""
    
    # åˆ›å»ºçƒ­åŠ›å›¾
    fig, ax = plt.subplots(figsize=(10, 8))
    
    # å¦‚æœæœ‰å…‹é‡Œé‡‘ç»“æœï¼Œä½¿ç”¨å…‹é‡Œé‡‘æ’å€¼
    if kriging_result is not None and 'grid_z' in kriging_result:
        # ä½¿ç”¨å…‹é‡Œé‡‘æ’å€¼ç»“æœ
        im = ax.imshow(kriging_result['grid_z'].T, 
                      extent=kriging_result['extent'],
                      origin='lower', 
                      cmap='YlOrRd',
                      alpha=0.8)
        
        # æ·»åŠ åŸå§‹æ•°æ®ç‚¹
        ax.scatter(data['X'], data['Y'], c='black', s=40, alpha=0.8, 
                  edgecolors='white', linewidth=1, zorder=5)
        
        title = f'{element} Kriging Interpolation Heatmap'
    else:
        # ä½¿ç”¨åŸå§‹æ•°æ®ç‚¹åˆ›å»ºç®€å•çƒ­åŠ›å›¾
        from scipy.interpolate import griddata
        
        # åˆ›å»ºç½‘æ ¼
        xi = np.linspace(data['X'].min(), data['X'].max(), 100)
        yi = np.linspace(data['Y'].min(), data['Y'].max(), 100)
        xi_grid, yi_grid = np.meshgrid(xi, yi)
        
        # æ’å€¼
        zi = griddata((data['X'], data['Y']), data[element], 
                     (xi_grid, yi_grid), method='cubic')
        
        # ç»˜åˆ¶çƒ­åŠ›å›¾
        im = ax.contourf(xi_grid, yi_grid, zi, levels=20, cmap='YlOrRd', alpha=0.8)
        
        # æ·»åŠ åŸå§‹æ•°æ®ç‚¹
        scatter = ax.scatter(data['X'], data['Y'], c=data[element], 
                           cmap='YlOrRd', s=50, alpha=0.9, 
                           edgecolors='black', linewidth=1, zorder=5)
        
        title = f'{element} Distribution Heatmap'
    
    # è®¾ç½®å›¾è¡¨å±æ€§
    ax.set_xlabel('Longitude (Â°)', fontsize=12)
    ax.set_ylabel('Latitude (Â°)', fontsize=12)
    ax.set_title(title, fontsize=14, fontweight='bold')
    
    # æ·»åŠ é¢œè‰²æ¡
    plt.colorbar(im, ax=ax, label=f'{element} (ppm)')
    
    # æ·»åŠ ç½‘æ ¼
    ax.grid(True, alpha=0.3)
    
    # å¦‚æœæœ‰é˜ˆå€¼ï¼Œæ·»åŠ å¼‚å¸¸åŒºåŸŸ
    if threshold is not None and kriging_result is not None and 'grid_z' in kriging_result:
        # åˆ›å»ºå¼‚å¸¸åŒºåŸŸæ©ç 
        anomaly_mask = kriging_result['grid_z'] > threshold
        
        # ç»˜åˆ¶å¼‚å¸¸åŒºåŸŸè½®å»“
        ax.contour(kriging_result['grid_x'], kriging_result['grid_y'], 
                  anomaly_mask.T, levels=[0.5], colors='red', linewidths=2, 
                  linestyles='--', label=f'Anomaly Threshold: {threshold:.3f}')
        ax.legend()
    
    plt.tight_layout()
    st.pyplot(fig)
    plt.close(fig)
    
    # æ·»åŠ AIè§£é‡ŠæŒ‰é’®
    if st.button("ğŸ¤– AIè§£é‡Šçƒ­åŠ›å›¾", key="explain_heatmap"):
        with st.spinner("ğŸ¤– AIæ­£åœ¨ç”Ÿæˆè§£é‡Š..."):
            element_data = data[element].dropna()
            
            explanation_prompt = f"""
è¯·è§£é‡Šä»¥ä¸‹åœ°çƒåŒ–å­¦å…ƒç´ çƒ­åŠ›å›¾åˆ†æç»“æœï¼š

## çƒ­åŠ›å›¾åˆ†æç»“æœ
- åˆ†æå…ƒç´ : {element}
- æ ·æœ¬æ•°é‡: {len(element_data)}
- æ•°æ®èŒƒå›´: X=[{data['X'].min():.3f}, {data['X'].max():.3f}], Y=[{data['Y'].min():.3f}, {data['Y'].max():.3f}]
- å…ƒç´ æµ“åº¦èŒƒå›´: [{element_data.min():.3f}, {element_data.max():.3f}] ppm
- å¹³å‡å€¼: {element_data.mean():.3f} ppm
"""
            
            if kriging_result is not None and 'grid_z' in kriging_result:
                explanation_prompt += f"""
## å…‹é‡Œé‡‘æ’å€¼ä¿¡æ¯
- æ’å€¼æ–¹æ³•: å…‹é‡Œé‡‘æ’å€¼
- ç½‘æ ¼åˆ†è¾¨ç‡: {len(kriging_result['grid_x'])}x{len(kriging_result['grid_y'])}
- æ’å€¼èŒƒå›´: [{kriging_result['extent'][0]:.3f}, {kriging_result['extent'][1]:.3f}]
"""
            
            if threshold is not None:
                anomaly_count = (element_data > threshold).sum()
                explanation_prompt += f"""
## å¼‚å¸¸åˆ†æ
- å¼‚å¸¸é˜ˆå€¼: {threshold:.3f} ppm
- å¼‚å¸¸æ ·å“æ•°: {anomaly_count}
- å¼‚å¸¸ç‡: {anomaly_count/len(element_data)*100:.1f}%
"""
            
            explanation_prompt += f"""

è¯·ä»åœ°è´¨å­¦è§’åº¦è§£é‡Šï¼š
1. çƒ­åŠ›å›¾ä¸­å…ƒç´ åˆ†å¸ƒçš„ç©ºé—´ç‰¹å¾
2. é«˜å€¼åŒºå’Œä½å€¼åŒºçš„åœ°è´¨æ„ä¹‰
3. ç©ºé—´åˆ†å¸ƒæ¨¡å¼ä¸åœ°è´¨æ„é€ çš„å…³ç³»
4. å¼‚å¸¸åŒºåŸŸçš„æˆçŸ¿æ½œåŠ›è¯„ä»·
5. å¯¹å‹˜æ¢é¶åŒºä¼˜é€‰çš„æŒ‡å¯¼æ„ä¹‰

è¯·ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€è§£é‡Šï¼Œä¾¿äºåœ°è´¨å‹˜æ¢äººå‘˜ç†è§£ã€‚
"""
            
            api_key = st.session_state.get('deepseek_api_key', '')
            explanation = call_deepseek_api(explanation_prompt, api_key)
            
            if not explanation.startswith("âŒ"):
                st.markdown("**ğŸ§  AIåœ°è´¨è§£é‡Šï¼š**")
                st.markdown(explanation)
            else:
                st.error(explanation)

def create_interactive_map(data, element, threshold=None):
    """åˆ›å»ºäº¤äº’å¼åœ°å›¾"""
    # è®¡ç®—ä¸­å¿ƒç‚¹
    center_lat = data['Y'].mean()
    center_lon = data['X'].mean()
    
    # åˆ›å»ºåœ°å›¾ï¼ˆæ— åº•å›¾ï¼‰
    m = folium.Map(
        location=[center_lat, center_lon],
        zoom_start=10,
        tiles=None
    )
    
    # æ·»åŠ é‡‡æ ·ç‚¹
    for idx, row in data.iterrows():
        color = 'red' if row.get('Is_Deposit', 0) == 1 else 'blue'
        size = 8 if row.get('Is_Deposit', 0) == 1 else 5
        
        folium.CircleMarker(
            location=[row['Y'], row['X']],
            radius=size,
            popup=f"ç‚¹ä½ {idx}<br>{element}: {row[element]:.3f}",
            color=color,
            fill=True,
            fillColor=color,
            fillOpacity=0.7
        ).add_to(m)
    
    # å¦‚æœæœ‰é˜ˆå€¼ï¼Œæ·»åŠ å¼‚å¸¸åŒºåŸŸ
    if threshold is not None:
        anomaly_points = data[data[element] > threshold]
        if len(anomaly_points) > 0:
            # åˆ›å»ºå¼‚å¸¸åŒºåŸŸçš„å‡¸åŒ…
            from scipy.spatial import ConvexHull
            points = anomaly_points[['Y', 'X']].values
            
            if len(points) >= 3:
                try:
                    hull = ConvexHull(points)
                    hull_points = points[hull.vertices]
                    
                    # åˆ›å»ºå¤šè¾¹å½¢
                    folium.Polygon(
                        locations=[[p[0], p[1]] for p in hull_points],
                        color='red',
                        fill=True,
                        fillColor='red',
                        fillOpacity=0.2,
                        popup='å¼‚å¸¸åŒºåŸŸ'
                    ).add_to(m)
                except:
                    pass
    
    return m

# æ¨¡æ‹ŸAgentå“åº”
def mock_agent_response(user_input):
    """æ¨¡æ‹ŸAgentå“åº”"""
    responses = {
        "ç›¸å…³æ€§": "æˆ‘æ­£åœ¨åˆ†æå…ƒç´ ä¹‹é—´çš„ç›¸å…³æ€§ã€‚æ ¹æ®è®¡ç®—ç»“æœï¼ŒAuä¸Asçš„ç›¸å…³ç³»æ•°ä¸º0.75ï¼Œæ˜¾ç¤ºå‡ºå¼ºçƒˆçš„æ­£ç›¸å…³æ€§ï¼Œè¿™æ˜¯é‡‘çŸ¿æˆçŸ¿çš„é‡è¦åœ°çƒåŒ–å­¦æŒ‡æ ‡ã€‚",
        "å¼‚å¸¸": "æˆ‘å·²ç»å®Œæˆäº†æ™ºèƒ½å¼‚å¸¸æ£€æµ‹åˆ†æï¼Œè¯†åˆ«å‡ºAuçš„å¼‚å¸¸é˜ˆå€¼ä¸º1.2 ppbï¼Œå…±æœ‰15ä¸ªæ ·å“è¢«å½’ç±»ä¸ºå¼‚å¸¸ï¼Œè¿™äº›åŒºåŸŸå€¼å¾—è¿›ä¸€æ­¥å‹˜æ¢ã€‚",
        "èšç±»": "åŸºäºæœºå™¨å­¦ä¹ çš„èšç±»åˆ†ææ˜¾ç¤ºï¼ŒAuã€Asã€Sbã€Hgå½¢æˆä¸€ä¸ªç´§å¯†çš„å…ƒç´ ç»„åˆï¼Œè¿™æ˜¯å…¸å‹çš„é‡‘çŸ¿åŒ–å…ƒç´ ç»„åˆç‰¹å¾ã€‚",
        "é¢„æµ‹": "é€šè¿‡èåˆåœ°è´¨çŸ¥è¯†å›¾è°±ä¸å¤§æ¨¡å‹çš„æ™ºèƒ½é¢„æµ‹ç³»ç»Ÿï¼Œç ”ç©¶åŒºçš„æˆçŸ¿æ½œåŠ›è¯„åˆ†ä¸º0.75ï¼Œå±äºé«˜æ½œåŠ›åŒºåŸŸã€‚",
        "å‹˜æ¢": "æ ¹æ®æ™ºèƒ½ä½“åˆ†æï¼Œå»ºè®®é‡ç‚¹å…³æ³¨æ„é€ æ–­è£‚å¸¦é™„è¿‘çš„å¼‚å¸¸åŒºåŸŸï¼Œè¿™äº›åŒºåŸŸå…·æœ‰è¾ƒå¥½çš„æˆçŸ¿åœ°è´¨æ¡ä»¶ã€‚",
        "æ¨¡å‹": "æœ¬å¹³å°é‡‡ç”¨å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œèåˆäº†åœ°è´¨å­¦ã€åœ°çƒåŒ–å­¦ã€é¥æ„Ÿç­‰å¤šæºæ•°æ®ï¼Œæä¾›ç²¾å‡†çš„é‡‘çŸ¿é¢„æµ‹æœåŠ¡ã€‚"
    }
    
    for key, response in responses.items():
        if key in user_input:
            return response
    
    return "æˆ‘æ˜¯é‡‘çŸ¿æ™ºèƒ½é¢„æµ‹ä¸“å®¶ï¼Œæ­£åœ¨åˆ†ææ‚¨çš„è¯·æ±‚ã€‚æˆ‘å¯ä»¥ä¸ºæ‚¨æä¾›æˆçŸ¿é¢„æµ‹ã€å¼‚å¸¸è¯†åˆ«ã€å‹˜æ¢å»ºè®®ç­‰ä¸“ä¸šæœåŠ¡ã€‚"

# ä¾§è¾¹æ é…ç½®
def render_sidebar():
    """æ¸²æŸ“ä¾§è¾¹æ """
    st.sidebar.markdown("""
    <div style='text-align: center; padding: 20px;'>
        <h1>â›ï¸ Gold-Seeker</h1>
        <p style='font-size: 14px; opacity: 0.8;'>é‡‘çŸ¿æ™ºèƒ½é¢„æµ‹æ™ºèƒ½ä½“å¹³å°</p>
        <p style='font-size: 12px; opacity: 0.6;'>èåˆé¢†åŸŸçŸ¥è¯†ä¸å¤§æ¨¡å‹æŠ€æœ¯</p>
    </div>
    """, unsafe_allow_html=True)
    
    # æ•°æ®ä¸Šä¼ 
    st.sidebar.markdown("### ğŸ“ æ•°æ®ä¸Šä¼ ")
    uploaded_file = st.sidebar.file_uploader(
        "é€‰æ‹©CSVæˆ–GeoJSONæ–‡ä»¶",
        type=['csv', 'geojson'],
        help="ä¸Šä¼ åœ°çƒåŒ–å­¦æ•°æ®æ–‡ä»¶"
    )
    
    if uploaded_file is not None:
        try:
            if uploaded_file.name.endswith('.csv'):
                data = pd.read_csv(uploaded_file)
            else:
                # ç®€å•çš„GeoJSONå¤„ç†
                import geopandas as gpd
                gdf = gpd.read_file(uploaded_file)
                data = pd.DataFrame(gdf.drop(columns='geometry'))
            
            st.session_state.data = data
            st.sidebar.success(f"âœ… æˆåŠŸåŠ è½½æ•°æ®: {data.shape}")
        except Exception as e:
            st.sidebar.error(f"âŒ åŠ è½½å¤±è´¥: {str(e)}")
    
    # ä½¿ç”¨ç¤ºä¾‹æ•°æ®
    if st.sidebar.button("ğŸ² ä½¿ç”¨ç¤ºä¾‹æ•°æ®"):
        st.session_state.data = generate_mock_data()
        st.sidebar.success("âœ… å·²åŠ è½½ç¤ºä¾‹æ•°æ®")
    
    # å‚æ•°è®¾ç½®
    st.sidebar.markdown("### âš™ï¸ å‚æ•°è®¾ç½®")
    
    # é€‰æ‹©ç›®æ ‡çŸ¿ç§
    target_mineral = st.sidebar.selectbox(
        "ç›®æ ‡çŸ¿ç§",
        ['Au', 'Ag', 'Cu', 'Pb', 'Zn'],
        index=0,
        help="é€‰æ‹©ä¸»è¦æ‰¾çŸ¿ç›®æ ‡å…ƒç´ "
    )
    st.session_state.target_mineral = target_mineral
    
    # é€‰æ‹©åˆ†æå…ƒç´ 
    if st.session_state.data is not None:
        available_elements = [col for col in st.session_state.data.columns 
                           if col not in ['X', 'Y', 'Is_Deposit']]
        
        selected_elements = st.sidebar.multiselect(
            "åˆ†æå…ƒç´ ",
            available_elements,
            default=['Au', 'As', 'Sb', 'Hg'] if all(e in available_elements for e in ['Au', 'As', 'Sb', 'Hg']) else available_elements[:4],
            help="é€‰æ‹©è¦åˆ†æçš„å…ƒç´ "
        )
        st.session_state.selected_elements = selected_elements
    
    # åˆå§‹åŒ–Agent
    st.sidebar.markdown("### ğŸ¤– åˆå§‹åŒ–æ™ºèƒ½ä½“")
    if st.sidebar.button("ğŸš€ Initialize Agent", type="primary"):
        if st.session_state.data is not None:
            # TODO: æ›¿æ¢ä¸ºçœŸå®çš„SpatialAnalystAgentåˆå§‹åŒ–
            st.session_state.agent = "Mock Agent"
            st.sidebar.success("âœ… Agentå·²åˆå§‹åŒ–")
        else:
            st.sidebar.error("âŒ è¯·å…ˆä¸Šä¼ æ•°æ®")

# AgentèŠå¤©ç•Œé¢
def render_agent_chat():
    """æ¸²æŸ“AgentèŠå¤©ç•Œé¢"""
    st.markdown("### ğŸ¤– é‡‘çŸ¿æ™ºèƒ½é¢„æµ‹å¯¹è¯")
    st.markdown("""
    <div style='background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px; margin-bottom: 20px;'>
        <p>ğŸ¤– <strong>æ™ºèƒ½ä½“ä»‹ç»ï¼š</strong>æˆ‘æ˜¯èåˆåœ°è´¨é¢†åŸŸçŸ¥è¯†ä¸å…ˆè¿›å¤§æ¨¡å‹æŠ€æœ¯çš„é‡‘çŸ¿æ™ºèƒ½é¢„æµ‹ä¸“å®¶ï¼Œ
        èƒ½å¤Ÿä¸ºæ‚¨æä¾›ä¸“ä¸šçš„é‡‘çŸ¿å‹˜æ¢å»ºè®®ã€æ•°æ®åˆ†æå’ŒæˆçŸ¿é¢„æµ‹æœåŠ¡ã€‚</p>
    </div>
    """, unsafe_allow_html=True)
    
    # æ˜¾ç¤ºèŠå¤©å†å²
    for message in st.session_state.chat_history:
        if message['role'] == 'user':
            st.markdown(f"""
            <div class="chat-message user-message">
                <strong>ğŸ‘¤ ç”¨æˆ·:</strong> {message['content']}
            </div>
            """, unsafe_allow_html=True)
        else:
            st.markdown(f"""
            <div class="chat-message agent-message">
                <strong>ğŸ¤– Agent:</strong> {message['content']}
            </div>
            """, unsafe_allow_html=True)
    
    # ç”¨æˆ·è¾“å…¥
    user_input = st.text_input("ğŸ’¬ è¾“å…¥æ‚¨çš„é—®é¢˜:", key="user_input")
    
    if st.button("ğŸ“¤ å‘é€") and user_input:
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        st.session_state.chat_history.append({
            'role': 'user',
            'content': user_input
        })
        
        # æ¨¡æ‹ŸAgentå“åº”
        # TODO: æ›¿æ¢ä¸ºçœŸå®çš„SpatialAnalystAgentè°ƒç”¨
        agent_response = mock_agent_response(user_input)
        
        # æ·»åŠ Agentå“åº”
        st.session_state.chat_history.append({
            'role': 'agent',
            'content': agent_response
        })
        
        # æ¸…ç©ºè¾“å…¥æ¡†
        st.session_state.user_input = ""
        
        # é‡æ–°è¿è¡Œä»¥æ˜¾ç¤ºæ–°æ¶ˆæ¯
        st.rerun()
    
    # æ¸…ç©ºèŠå¤©å†å²
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºèŠå¤©å†å²"):
        st.session_state.chat_history = []
        st.rerun()

# æ•°æ®åˆ†æç•Œé¢
def render_data_analysis():
    """æ¸²æŸ“æ•°æ®åˆ†æç•Œé¢"""
    st.markdown("### ğŸ“Š æ•°æ®é¢„è§ˆ")
    
    if st.session_state.data is not None:
        data = st.session_state.data
        elements = st.session_state.selected_elements
        
        # æ•°æ®æ¦‚è§ˆ
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("æ ·æœ¬æ•°é‡", len(data))
        with col2:
            st.metric("åˆ†æå…ƒç´ ", len(elements))
        with col3:
            st.metric("ç›®æ ‡çŸ¿ç§", st.session_state.target_mineral)
        
        # æ•°æ®è¡¨æ ¼
        st.markdown("#### ğŸ“‹ æ•°æ®è¡¨æ ¼")
        st.dataframe(data.head(10))
        
        # ç»Ÿè®¡ä¿¡æ¯
        st.markdown("#### ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯")
        if elements:
            stats_data = data[elements].describe()
            st.dataframe(stats_data)
        
        # å¯è§†åŒ–åŒºåŸŸ
        st.markdown("#### ğŸ“Š å¯è§†åŒ–åˆ†æ")
        
        if len(elements) >= 2:
            # ç›¸å…³æ€§çƒ­åŠ›å›¾
            with st.expander("ğŸ”¥ ç›¸å…³æ€§çƒ­åŠ›å›¾", expanded=True):
                fig = create_correlation_heatmap(data, elements)
                st.pyplot(fig)
                plt.close()
                
                # æ·»åŠ AIè§£é‡ŠæŒ‰é’®
                if st.button("ğŸ¤– AIè§£é‡Šç›¸å…³æ€§çƒ­åŠ›å›¾", key="explain_correlation"):
                    with st.spinner("ğŸ¤– AIæ­£åœ¨ç”Ÿæˆè§£é‡Š..."):
                        correlation_matrix = data[elements].corr()
                        explanation_prompt = f"""
è¯·è§£é‡Šä»¥ä¸‹åœ°çƒåŒ–å­¦å…ƒç´ ç›¸å…³æ€§çƒ­åŠ›å›¾çš„åˆ†æç»“æœï¼š

## ç›¸å…³æ€§çŸ©é˜µ
{correlation_matrix.round(3).to_string()}

## åˆ†æå…ƒç´ 
{', '.join(elements)}

è¯·ä»åœ°è´¨å­¦è§’åº¦è§£é‡Šï¼š
1. å…ƒç´ é—´çš„ç›¸å…³æ€§å¼ºåº¦å’Œæ–¹å‘
2. é«˜ç›¸å…³æ€§å…ƒç´ çš„åœ°è´¨æ„ä¹‰
3. è´Ÿç›¸å…³æ€§å…ƒç´ çš„æˆå› è§£é‡Š
4. å¯¹é‡‘çŸ¿å‹˜æ¢çš„æŒ‡ç¤ºæ„ä¹‰

è¯·ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€è§£é‡Šï¼Œä¾¿äºåœ°è´¨å‹˜æ¢äººå‘˜ç†è§£ã€‚
"""
                        
                        api_key = st.session_state.get('deepseek_api_key', '')
                        explanation = call_deepseek_api(explanation_prompt, api_key)
                        
                        if not explanation.startswith("âŒ"):
                            st.markdown("**ğŸ§  AIåœ°è´¨è§£é‡Šï¼š**")
                            st.markdown(explanation)
                        else:
                            st.error(explanation)
            
            # Rå‹èšç±»æ ‘çŠ¶å›¾
            with st.expander("ğŸŒ³ Rå‹èšç±»æ ‘çŠ¶å›¾", expanded=True):
                fig = create_dendrogram(data, elements)
                st.pyplot(fig)
                plt.close()
                
                # æ·»åŠ AIè§£é‡ŠæŒ‰é’®
                if st.button("ğŸ¤– AIè§£é‡Šèšç±»åˆ†æ", key="explain_clustering"):
                    with st.spinner("ğŸ¤– AIæ­£åœ¨ç”Ÿæˆè§£é‡Š..."):
                        clustering_result = analyze_clustering_results(data, elements)
                        
                        explanation_prompt = f"""
è¯·è§£é‡Šä»¥ä¸‹åœ°çƒåŒ–å­¦å…ƒç´ Rå‹èšç±»åˆ†æç»“æœï¼š

## èšç±»åˆ†æç»“æœ
- æ ·æœ¬æ•°é‡: {clustering_result['n_samples']}
- ç‰¹å¾æ•°é‡: {clustering_result['n_features']}
- èšç±»æ–¹æ³•: Wardå±‚æ¬¡èšç±»
- è·ç¦»åº¦é‡: æ¬§å‡ é‡Œå¾—è·ç¦»

## åˆ†æå…ƒç´ 
{', '.join(elements)}

è¯·ä»åœ°è´¨å­¦è§’åº¦è§£é‡Šï¼š
1. å…ƒç´ èšç±»çš„ä¸»è¦åˆ†ç»„ç‰¹å¾
2. å„èšç±»ç»„çš„åœ°çƒåŒ–å­¦æ„ä¹‰
3. å…ƒç´ ç»„åˆçš„åœ°è´¨æˆå› è§£é‡Š
4. å¯¹é‡‘çŸ¿å‹˜æ¢çš„æŒ‡å¯¼æ„ä¹‰

è¯·ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€è§£é‡Šï¼Œä¾¿äºåœ°è´¨å‹˜æ¢äººå‘˜ç†è§£ã€‚
"""
                        
                        api_key = st.session_state.get('deepseek_api_key', '')
                        explanation = call_deepseek_api(explanation_prompt, api_key)
                        
                        if not explanation.startswith("âŒ"):
                            st.markdown("**ğŸ§  AIåœ°è´¨è§£é‡Šï¼š**")
                            st.markdown(explanation)
                        else:
                            st.error(explanation)
            
            # PCAè½½è·å›¾
            with st.expander("ğŸ¯ PCAè½½è·å›¾", expanded=True):
                fig = create_pca_loadings_plot(data, elements)
                st.pyplot(fig)
                plt.close()
                
                # æ·»åŠ AIè§£é‡ŠæŒ‰é’®
                if st.button("ğŸ¤– AIè§£é‡ŠPCAåˆ†æ", key="explain_pca"):
                    with st.spinner("ğŸ¤– AIæ­£åœ¨ç”Ÿæˆè§£é‡Š..."):
                        pca_result = analyze_pca_results(data, elements)
                        
                        explanation_prompt = f"""
è¯·è§£é‡Šä»¥ä¸‹åœ°çƒåŒ–å­¦å…ƒç´ PCAä¸»æˆåˆ†åˆ†æç»“æœï¼š

## PCAåˆ†æç»“æœ
- ä¸»æˆåˆ†1è§£é‡Šæ–¹å·®: {pca_result['explained_variance'][0]:.3f} ({pca_result['explained_variance'][0]*100:.1f}%)
- ä¸»æˆåˆ†2è§£é‡Šæ–¹å·®: {pca_result['explained_variance'][1]:.3f} ({pca_result['explained_variance'][1]*100:.1f}%)
- ç´¯ç§¯è§£é‡Šæ–¹å·®: {pca_result['cumulative_variance'][1]:.3f} ({pca_result['cumulative_variance'][1]*100:.1f}%)

## ä¸»æˆåˆ†è½½è·
"""
                        
                        for i, element in enumerate(elements):
                            explanation_prompt += f"""
- {element}: PC1={pca_result['loadings'][i][0]:.3f}, PC2={pca_result['loadings'][i][1]:.3f}
"""
                        
                        explanation_prompt += f"""

## åˆ†æå…ƒç´ 
{', '.join(elements)}

è¯·ä»åœ°è´¨å­¦è§’åº¦è§£é‡Šï¼š
1. ä¸»æˆåˆ†çš„åœ°çƒåŒ–å­¦æ„ä¹‰
2. é«˜è½½è·å…ƒç´ çš„åœ°è´¨æŒ‡ç¤º
3. ä¸»æˆåˆ†ç»„åˆçš„æˆå› è§£é‡Š
4. å¯¹é‡‘çŸ¿å‹˜æ¢çš„åº”ç”¨ä»·å€¼

è¯·ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€è§£é‡Šï¼Œä¾¿äºåœ°è´¨å‹˜æ¢äººå‘˜ç†è§£ã€‚
"""
                        
                        api_key = st.session_state.get('deepseek_api_key', '')
                        explanation = call_deepseek_api(explanation_prompt, api_key)
                        
                        if not explanation.startswith("âŒ"):
                            st.markdown("**ğŸ§  AIåœ°è´¨è§£é‡Šï¼š**")
                            st.markdown(explanation)
                        else:
                            st.error(explanation)
            
            # åœ°è´¨è§£é‡Šé¢æ¿
            st.markdown("#### ğŸ§  AIåœ°è´¨è§£é‡Š")
            create_geological_interpretation_panel(data, elements)
        else:
            st.warning("âš ï¸ è¯·è‡³å°‘é€‰æ‹©2ä¸ªå…ƒç´ è¿›è¡Œåˆ†æ")
    else:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æ•°æ®")

# ç©ºé—´åˆ†æç•Œé¢
def render_spatial_analysis():
    """æ¸²æŸ“ç©ºé—´åˆ†æç•Œé¢"""
    st.markdown("### ğŸ—ºï¸ ç©ºé—´åˆ†æ")
    
    if st.session_state.data is not None:
        data = st.session_state.data
        target_element = st.session_state.target_mineral
        
        # é€‰æ‹©åˆ†æå…ƒç´ 
        analysis_element = st.selectbox(
            "é€‰æ‹©åˆ†æå…ƒç´ ",
            st.session_state.selected_elements,
            index=0 if st.session_state.selected_elements else 0
        )
        
        # åˆ†æé€‰é¡¹
        col1, col2, col3 = st.columns(3)
        with col1:
            show_comprehensive = st.checkbox("ğŸ“Š ç»¼åˆåˆ†æ", value=True)
        with col2:
            show_variogram = st.checkbox("ğŸ“ˆ å˜å·®å‡½æ•°", value=True)
        with col3:
            show_3d = st.checkbox("ğŸ¯ 3Då¯è§†åŒ–", value=False)
        
        # ç»¼åˆåˆ†æé¢æ¿
        if show_comprehensive:
            st.markdown("#### ğŸ“Š ç»¼åˆåœ°çƒåŒ–å­¦åˆ†æ")
            create_comprehensive_analysis_panel(data, analysis_element)
        
        # C-Aåˆ†å½¢åˆ†æ
        st.markdown("#### ğŸ“ˆ C-A Fractal Analysis")
        
        # åˆå§‹åŒ–thresholdå˜é‡
        threshold = None
        
        with st.expander("ğŸ” C-A Fractal Plot", expanded=True):
            processor = GeochemProcessor()
            ca_result = processor.run_fractal_ca_model(data, analysis_element)
            
            st.pyplot(ca_result['figure'])
            plt.close(ca_result['figure'])
            
            if ca_result['threshold_value']:
                threshold = ca_result['threshold_value']
                st.info(f"ğŸ“ Calculated Anomaly Threshold: {threshold:.3f}")
                
                # æ·»åŠ AIè§£é‡ŠæŒ‰é’®
                if st.button("ğŸ¤– AIè§£é‡ŠC-Aåˆ†å½¢åˆ†æ", key="explain_ca_fractal"):
                    with st.spinner("ğŸ¤– AIæ­£åœ¨ç”Ÿæˆè§£é‡Š..."):
                        element_data = data[analysis_element].dropna()
                        
                        explanation_prompt = f"""
è¯·è§£é‡Šä»¥ä¸‹åœ°çƒåŒ–å­¦C-Aåˆ†å½¢åˆ†æç»“æœï¼š

## C-Aåˆ†å½¢åˆ†æç»“æœ
- åˆ†æå…ƒç´ : {analysis_element}
- å¼‚å¸¸é˜ˆå€¼: {threshold:.3f}
- æ ·æœ¬æ•°é‡: {len(element_data)}
- æ•°æ®èŒƒå›´: [{element_data.min():.3f}, {element_data.max():.3f}]
- å¹³å‡å€¼: {element_data.mean():.3f}
- æ ‡å‡†å·®: {element_data.std():.3f}

## å¼‚å¸¸ç»Ÿè®¡
- å¼‚å¸¸æ ·å“æ•°: {(element_data > threshold).sum()}
- å¼‚å¸¸ç‡: {(element_data > threshold).sum()/len(element_data)*100:.1f}%

è¯·ä»åœ°è´¨å­¦è§’åº¦è§£é‡Šï¼š
1. C-Aåˆ†å½¢åˆ†æçš„åœ°è´¨æ„ä¹‰
2. å¼‚å¸¸é˜ˆå€¼çš„åˆç†æ€§è¯„ä»·
3. å¼‚å¸¸åˆ†å¸ƒçš„ç©ºé—´åˆ†å¸ƒç‰¹å¾
4. å¯¹é‡‘çŸ¿å‹˜æ¢çš„æŒ‡å¯¼æ„ä¹‰
5. ä¸‹ä¸€æ­¥å‹˜æ¢å»ºè®®

è¯·ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€è§£é‡Šï¼Œä¾¿äºåœ°è´¨å‹˜æ¢äººå‘˜ç†è§£ã€‚
"""
                        
                        api_key = st.session_state.get('deepseek_api_key', '')
                        explanation = call_deepseek_api(explanation_prompt, api_key)
                        
                        if not explanation.startswith("âŒ"):
                            st.markdown("**ğŸ§  AIåœ°è´¨è§£é‡Šï¼š**")
                            st.markdown(explanation)
                        else:
                            st.error(explanation)
        
        # å…‹é‡Œé‡‘æ’å€¼çƒ­åŠ›å›¾
        st.markdown("#### ğŸ”¥ Advanced Kriging Interpolation")
        
        # åˆå§‹åŒ–å…‹é‡Œé‡‘ç»“æœå˜é‡
        kriging_result = None
        
        if st.button("Generate Kriging Heatmap", type="primary"):
            with st.spinner("Generating kriging interpolation..."):
                try:
                    processor = GeochemProcessor()
                    kriging_result = processor.interpolate_kriging(
                        data, 
                        target_element=analysis_element,
                        grid_resolution=0.01
                    )
                    
                    # åˆ›å»ºä¸“ä¸šå…‹é‡Œé‡‘åˆ†æå±•ç¤º
                    create_professional_kriging_display(data, analysis_element, kriging_result, threshold)
                    
                    # æ˜¾ç¤ºæ’å€¼ç»Ÿè®¡
                    st.success("âœ… å…‹é‡Œé‡‘æ’å€¼å®Œæˆ!")
                    
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("ç½‘æ ¼åˆ†è¾¨ç‡", f"{len(kriging_result['grid_x'])}x{len(kriging_result['grid_y'])}")
                    with col2:
                        st.metric("æ’å€¼èŒƒå›´", f"[{kriging_result['extent'][0]:.2f}, {kriging_result['extent'][1]:.2f}]")
                    with col3:
                        st.metric("æœ‰æ•ˆæ•°æ®ç‚¹", len(kriging_result['points']['x']))
                    with col4:
                        if 'variogram_params' in kriging_result:
                            range_val = kriging_result['variogram_params'].get('range', 'N/A')
                            st.metric("å˜ç¨‹", f"{range_val:.3f}" if range_val != 'N/A' else 'N/A')
                    
                    # æ·»åŠ AIè§£é‡ŠæŒ‰é’®
                    if st.button("ğŸ¤– AIè§£é‡Šå…‹é‡Œé‡‘æ’å€¼", key="explain_kriging"):
                        with st.spinner("ğŸ¤– AIæ­£åœ¨ç”Ÿæˆè§£é‡Š..."):
                            element_data = data[analysis_element].dropna()
                            
                            explanation_prompt = f"""
è¯·è§£é‡Šä»¥ä¸‹åœ°çƒåŒ–å­¦å…‹é‡Œé‡‘æ’å€¼åˆ†æç»“æœï¼š

## å…‹é‡Œé‡‘æ’å€¼ç»“æœ
- åˆ†æå…ƒç´ : {analysis_element}
- ç½‘æ ¼åˆ†è¾¨ç‡: {len(kriging_result['grid_x'])}x{len(kriging_result['grid_y'])}
- æ’å€¼èŒƒå›´: [{kriging_result['extent'][0]:.2f}, {kriging_result['extent'][1]:.2f}]
- æœ‰æ•ˆæ•°æ®ç‚¹: {len(kriging_result['points']['x'])}
"""
                            
                            if 'variogram_params' in kriging_result:
                                params = kriging_result['variogram_params']
                                explanation_prompt += f"""
## å˜å·®å‡½æ•°å‚æ•°
- å—é‡‘å€¼: {params.get('nugget', 'N/A')}
- åŸºå°å€¼: {params.get('sill', 'N/A')}
- å˜ç¨‹: {params.get('range', 'N/A')}
- æ¨¡å‹ç±»å‹: {params.get('model', 'N/A')}
"""
                            
                            explanation_prompt += f"""
## åŸå§‹æ•°æ®ç»Ÿè®¡
- æ ·æœ¬æ•°é‡: {len(element_data)}
- æ•°æ®èŒƒå›´: [{element_data.min():.3f}, {element_data.max():.3f}]
- å¹³å‡å€¼: {element_data.mean():.3f}
- æ ‡å‡†å·®: {element_data.std():.3f}

è¯·ä»åœ°è´¨å­¦è§’åº¦è§£é‡Šï¼š
1. å…‹é‡Œé‡‘æ’å€¼ç»“æœçš„å¯é æ€§è¯„ä»·
2. å˜å·®å‡½æ•°å‚æ•°çš„åœ°è´¨æ„ä¹‰
3. ç©ºé—´åˆ†å¸ƒç‰¹å¾å’Œè¿ç»­æ€§
4. æ’å€¼ç»“æœçš„å‹˜æ¢åº”ç”¨ä»·å€¼
5. å±€é™æ€§å’Œæ”¹è¿›å»ºè®®

è¯·ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€è§£é‡Šï¼Œä¾¿äºåœ°è´¨å‹˜æ¢äººå‘˜ç†è§£ã€‚
"""
                            
                            api_key = st.session_state.get('deepseek_api_key', '')
                            explanation = call_deepseek_api(explanation_prompt, api_key)
                            
                            if not explanation.startswith("âŒ"):
                                st.markdown("**ğŸ§  AIåœ°è´¨è§£é‡Šï¼š**")
                                st.markdown(explanation)
                            else:
                                st.error(explanation)
                    
                except Exception as e:
                    st.error(f"âŒ å…‹é‡Œé‡‘æ’å€¼å¤±è´¥: {str(e)}")
                    st.info("ğŸ’¡ æç¤º: è¯·ç¡®ä¿å®‰è£…äº† pykrige åŒ…: `pip install pykrige`")
                    kriging_result = None
        
        # ç©ºé—´åˆ†å¸ƒçƒ­åŠ›å›¾
        st.markdown("#### ğŸ—ºï¸ ç©ºé—´åˆ†å¸ƒçƒ­åŠ›å›¾")
        
        with st.expander("ğŸŒ çƒ­åŠ›å›¾å±•ç¤º", expanded=True):
            # ç¡®ä¿thresholdæœ‰é»˜è®¤å€¼
            if 'threshold' not in locals():
                threshold = None
            
            # åˆ›å»ºçƒ­åŠ›å›¾
            create_heatmap_display(data, analysis_element, threshold, kriging_result)
        
        # å¼‚å¸¸ç»Ÿè®¡
        if threshold:
            anomaly_count = (data[analysis_element] > threshold).sum()
            st.markdown("#### ğŸ“Š å¼‚å¸¸ç»Ÿè®¡")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("å¼‚å¸¸æ ·å“æ•°", anomaly_count)
            with col2:
                st.metric("å¼‚å¸¸ç‡", f"{anomaly_count/len(data)*100:.1f}%")
            with col3:
                st.metric("é˜ˆå€¼", f"{threshold:.3f}")
    else:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æ•°æ®")

# ä¸»å‡½æ•°
def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®è‡ªå®šä¹‰æ ·å¼
    set_custom_style()
    
    # åˆå§‹åŒ–session state
    init_session_state()
    
    # æ¸²æŸ“ä¾§è¾¹æ 
    render_sidebar()
    
    # ä¸»ç•Œé¢æ ‡é¢˜
    st.markdown("""
    <div style='text-align: center; padding: 30px 0;'>
        <h1>â›ï¸ Gold-Seeker: AI Mineral Prediction System</h1>
        <p style='font-size: 18px; opacity: 0.9;'>èåˆé¢†åŸŸçŸ¥è¯†ä¸å¤§æ¨¡å‹çš„é‡‘çŸ¿æ™ºèƒ½é¢„æµ‹æ™ºèƒ½ä½“å¹³å°</p>
    </div>
    """, unsafe_allow_html=True)
    
    # åˆ›å»ºæ ‡ç­¾é¡µ
    tab1, tab2, tab3 = st.tabs(["ğŸ¤– Agent Chat", "ğŸ“Š Data & R-mode Analysis", "ğŸ—ºï¸ Spatial & Anomaly"])
    
    with tab1:
        render_agent_chat()
    
    with tab2:
        render_data_analysis()
    
    with tab3:
        render_spatial_analysis()
    
    # é¡µè„š
    st.markdown("""
    <div style='text-align: center; padding: 20px; margin-top: 50px; border-top: 1px solid rgba(255,255,255,0.2);'>
        <p>Â© 2025 Gold-Seeker Development Team | èåˆé¢†åŸŸçŸ¥è¯†ä¸å¤§æ¨¡å‹çš„é‡‘çŸ¿æ™ºèƒ½é¢„æµ‹æ™ºèƒ½ä½“å¹³å°</p>
    </div>
    """, unsafe_allow_html=True)

# DeepSeek APIé…ç½®å’Œåœ°è´¨è§£é‡ŠåŠŸèƒ½
def get_deepseek_client():
    """è·å–DeepSeekå®¢æˆ·ç«¯"""
    api_key = st.session_state.get('deepseek_api_key', '')
    if not api_key:
        return None
    return api_key

def call_deepseek_api(prompt, api_key, max_retries=3):
    """è°ƒç”¨DeepSeek API"""
    if not api_key:
        return "âŒ è¯·å…ˆåœ¨è®¾ç½®ä¸­é…ç½®DeepSeek APIå¯†é’¥"
    
    for attempt in range(max_retries):
        try:
            url = "https://api.deepseek.com/v1/chat/completions"
            
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": "deepseek-chat",
                "messages": [
                    {
                        "role": "system", 
                        "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åœ°çƒåŒ–å­¦å®¶å’Œåœ°è´¨å‹˜æ¢ä¸“å®¶ï¼Œå…·æœ‰ä¸°å¯Œçš„é‡‘çŸ¿å‹˜æ¢ç»éªŒã€‚è¯·åŸºäºæä¾›çš„åœ°çƒåŒ–å­¦æ•°æ®åˆ†æç»“æœï¼Œç»™å‡ºä¸“ä¸šçš„åœ°è´¨è§£é‡Šå’Œå‹˜æ¢å»ºè®®ã€‚"
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                "temperature": 0.7,
                "max_tokens": 1500
            }
            
            # å¢åŠ è¶…æ—¶æ—¶é—´ï¼Œç¬¬ä¸€æ¬¡å°è¯•30ç§’ï¼Œåç»­å°è¯•60ç§’
            timeout = 30 if attempt == 0 else 60
            
            response = requests.post(url, headers=headers, json=data, timeout=timeout)
            
            if response.status_code == 200:
                result = response.json()
                return result['choices'][0]['message']['content']
            else:
                return f"APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}"
                
        except requests.exceptions.Timeout:
            if attempt < max_retries - 1:
                time.sleep(3)  # ç­‰å¾…3ç§’åé‡è¯•
                continue
            else:
                return "âŒ DeepSeek APIå“åº”è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•"
        except requests.exceptions.ConnectionError:
            if attempt < max_retries - 1:
                time.sleep(5)  # è¿æ¥é—®é¢˜ç­‰å¾…æ›´é•¿æ—¶é—´
                continue
            else:
                return "âŒ æ— æ³•è¿æ¥åˆ°DeepSeek APIï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                continue
            else:
                return f"è°ƒç”¨DeepSeek APIæ—¶å‡ºé”™: {str(e)}"

def analyze_pca_results(data, elements):
    """åˆ†æPCAç»“æœ"""
    try:
        # æ ‡å‡†åŒ–æ•°æ®
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(data[elements])
        
        # PCAåˆ†æ
        pca = PCA(n_components=min(3, len(elements)))
        pca_result = pca.fit_transform(scaled_data)
        
        # è·å–è½½è·çŸ©é˜µ
        loadings = pca.components_.T
        
        # è§£é‡Šæ–¹å·®
        explained_variance = pca.explained_variance_ratio_
        
        # åˆ›å»ºPCAç»“æœæ‘˜è¦
        pca_summary = {
            'explained_variance': explained_variance,
            'cumulative_variance': np.cumsum(explained_variance),
            'loadings': loadings,
            'components': pca_result
        }
        
        return pca_summary
        
    except Exception as e:
        return None

def analyze_clustering_results(data, elements):
    """åˆ†æèšç±»ç»“æœ"""
    try:
        # è®¡ç®—è·ç¦»çŸ©é˜µ
        distances = pdist(data[elements].values, metric='euclidean')
        
        # å±‚æ¬¡èšç±»
        linkage_matrix = linkage(distances, method='ward')
        
        # è·å–èšç±»ä¿¡æ¯
        clustering_info = {
            'linkage_matrix': linkage_matrix,
            'distance_matrix': distances,
            'n_samples': len(data),
            'n_features': len(elements)
        }
        
        return clustering_info
        
    except Exception as e:
        return None

def create_geological_interpretation_panel(data, elements):
    """åˆ›å»ºåœ°è´¨è§£é‡Šé¢æ¿"""
    
    # APIå¯†é’¥æ˜¾ç¤º
    st.markdown("##### ğŸ”‘ DeepSeek APIé…ç½®")
    
    # æ˜¾ç¤ºå·²é…ç½®çš„APIå¯†é’¥ï¼ˆéšè—éƒ¨åˆ†å­—ç¬¦ï¼‰
    api_key = st.session_state.get('deepseek_api_key', '')
    if api_key:
        masked_key = api_key[:8] + "..." + api_key[-4:]
        st.success(f"âœ… APIå¯†é’¥å·²é…ç½®: {masked_key}")
    else:
        st.error("âŒ APIå¯†é’¥æœªé…ç½®")
        return
    
    # åˆ†æé€‰é¡¹
    st.markdown("##### ğŸ§  AIåœ°è´¨è§£é‡Šåˆ†æ")
    
    analysis_options = st.multiselect(
        "é€‰æ‹©è¦åˆ†æçš„å†…å®¹:",
        ["PCAä¸»æˆåˆ†åˆ†æ", "èšç±»åˆ†æ", "ç›¸å…³æ€§åˆ†æ", "ç»Ÿè®¡ç‰¹å¾åˆ†æ"],
        default=["PCAä¸»æˆåˆ†åˆ†æ", "èšç±»åˆ†æ"]
    )
    
    if not analysis_options:
        st.info("ğŸ’¡ è¯·é€‰æ‹©è¦åˆ†æçš„å†…å®¹")
        return
    
    # ç”Ÿæˆåˆ†ææŒ‰é’®
    if st.button("ğŸš€ ç”Ÿæˆåœ°è´¨è§£é‡Š", type="primary"):
        with st.spinner("ğŸ¤– AIæ­£åœ¨åˆ†ææ•°æ®..."):
            
            # å‡†å¤‡åˆ†ææ•°æ®
            analysis_prompt = f"""
è¯·åŸºäºä»¥ä¸‹åœ°çƒåŒ–å­¦æ•°æ®è¿›è¡Œä¸“ä¸šçš„åœ°è´¨è§£é‡Šåˆ†æï¼š

## æ•°æ®åŸºæœ¬ä¿¡æ¯
- æ ·æœ¬æ•°é‡: {len(data)} ä¸ª
- åˆ†æå…ƒç´ : {', '.join(elements)}
- ç›®æ ‡çŸ¿ç§: {st.session_state.get('target_mineral', 'Au')}

## ç»Ÿè®¡ç‰¹å¾
"""
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            for element in elements:
                element_data = data[element].dropna()
                analysis_prompt += f"""
### {element} å…ƒç´ ç»Ÿè®¡
- å¹³å‡å€¼: {element_data.mean():.3f} ppm
- æ ‡å‡†å·®: {element_data.std():.3f} ppm
- æœ€å°å€¼: {element_data.min():.3f} ppm
- æœ€å¤§å€¼: {element_data.max():.3f} ppm
- ååº¦: {element_data.skew():.3f}
- å³°åº¦: {element_data.kurtosis():.3f}
"""
            
            # æ·»åŠ PCAåˆ†æ
            if "PCAä¸»æˆåˆ†åˆ†æ" in analysis_options and len(elements) >= 2:
                pca_result = analyze_pca_results(data, elements)
                if pca_result:
                    analysis_prompt += f"""

## PCAä¸»æˆåˆ†åˆ†æç»“æœ
- ä¸»æˆåˆ†1è§£é‡Šæ–¹å·®: {pca_result['explained_variance'][0]:.3f} ({pca_result['explained_variance'][0]*100:.1f}%)
- ä¸»æˆåˆ†2è§£é‡Šæ–¹å·®: {pca_result['explained_variance'][1]:.3f} ({pca_result['explained_variance'][1]*100:.1f}%)
- ç´¯ç§¯è§£é‡Šæ–¹å·®: {pca_result['cumulative_variance'][1]:.3f} ({pca_result['cumulative_variance'][1]*100:.1f}%)

### ä¸»æˆåˆ†è½½è·
"""
                    for i, element in enumerate(elements):
                        analysis_prompt += f"""
- {element}: PC1={pca_result['loadings'][i][0]:.3f}, PC2={pca_result['loadings'][i][1]:.3f}
"""
            
            # æ·»åŠ èšç±»åˆ†æ
            if "èšç±»åˆ†æ" in analysis_options and len(elements) >= 2:
                clustering_result = analyze_clustering_results(data, elements)
                if clustering_result:
                    analysis_prompt += f"""

## èšç±»åˆ†æç»“æœ
- æ ·æœ¬æ•°é‡: {clustering_result['n_samples']}
- ç‰¹å¾æ•°é‡: {clustering_result['n_features']}
- èšç±»æ–¹æ³•: Wardå±‚æ¬¡èšç±»
- è·ç¦»åº¦é‡: æ¬§å‡ é‡Œå¾—è·ç¦»
"""
            
            # æ·»åŠ ç›¸å…³æ€§åˆ†æ
            if "ç›¸å…³æ€§åˆ†æ" in analysis_options and len(elements) >= 2:
                correlation_matrix = data[elements].corr()
                analysis_prompt += f"""

## ç›¸å…³æ€§åˆ†æç»“æœ
### å…ƒç´ é—´ç›¸å…³ç³»æ•°
"""
                for i, element1 in enumerate(elements):
                    for j, element2 in enumerate(elements):
                        if i < j:
                            corr_value = correlation_matrix.loc[element1, element2]
                            analysis_prompt += f"""
- {element1} - {element2}: {corr_value:.3f}
"""
            
            # æ·»åŠ åœ°è´¨è§£é‡Šè¯·æ±‚
            analysis_prompt += """

## åœ°è´¨è§£é‡Šè¦æ±‚
è¯·åŸºäºä»¥ä¸Šæ•°æ®åˆ†æç»“æœï¼Œæä¾›ä¸“ä¸šçš„åœ°è´¨è§£é‡Šï¼ŒåŒ…æ‹¬ï¼š

1. **åœ°çƒåŒ–å­¦ç‰¹å¾è§£é‡Š**: 
   - å…ƒç´ åˆ†å¸ƒç‰¹å¾å’Œåœ°çƒåŒ–å­¦è¡Œä¸º
   - å…ƒç´ ç»„åˆå…³ç³»å’Œåœ°çƒåŒ–å­¦æ„ä¹‰

2. **åœ°è´¨æˆå› åˆ†æ**:
   - å¯èƒ½çš„çŸ¿åŒ–ç±»å‹å’ŒæˆçŸ¿ä½œç”¨
   - åœ°è´¨æ„é€ å’Œå²©æµ†æ´»åŠ¨å½±å“

3. **å‹˜æ¢æ„ä¹‰**:
   - åœ°çƒåŒ–å­¦å¼‚å¸¸çš„è¯†åˆ«å’Œè¯„ä»·
   - å‹˜æ¢é¶åŒºä¼˜é€‰å’Œæ‰¾çŸ¿æ–¹å‘

4. **ä¸‹ä¸€æ­¥å·¥ä½œå»ºè®®**:
   - éœ€è¦è¡¥å……çš„åˆ†ææµ‹è¯•
   - å‹˜æ¢æ–¹æ³•å’ŒæŠ€æœ¯è·¯çº¿

è¯·ä»¥ä¸“ä¸šåœ°è´¨å­¦å®¶çš„è§’åº¦è¿›è¡Œè§£é‡Šï¼Œæä¾›å®ç”¨çš„å‹˜æ¢å»ºè®®ã€‚
"""
            
            # è°ƒç”¨DeepSeek API
            api_key = st.session_state.get('deepseek_api_key', '')
            
            # æ·»åŠ ç½‘ç»œè¿æ¥æ£€æŸ¥
            try:
                # å…ˆæµ‹è¯•ç½‘ç»œè¿æ¥
                test_response = requests.get("https://www.baidu.com", timeout=5)
                network_ok = True
            except:
                network_ok = False
                st.error("âŒ ç½‘ç»œè¿æ¥å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®")
                return
            
            if network_ok:
                interpretation = call_deepseek_api(analysis_prompt, api_key)
                
                # æ˜¾ç¤ºç»“æœ
                st.markdown("##### ğŸ“‹ AIåœ°è´¨è§£é‡Šç»“æœ")
                
                if interpretation.startswith("âŒ") or interpretation.startswith("APIè°ƒç”¨å¤±è´¥"):
                    st.error(interpretation)
                    
                    # æä¾›é‡è¯•æŒ‰é’®
                    if st.button("ğŸ”„ é‡è¯•", key="retry_interpretation"):
                        st.rerun()
                else:
                    # ä½¿ç”¨markdownæ˜¾ç¤ºç»“æœ
                    st.markdown(interpretation)
                
                # æä¾›ä¸‹è½½é€‰é¡¹
                st.markdown("##### ğŸ’¾ å¯¼å‡ºç»“æœ")
                if st.button("ğŸ“„ ä¸‹è½½åœ°è´¨è§£é‡ŠæŠ¥å‘Š"):
                    report_content = f"""
# Gold-Seeker AIåœ°è´¨è§£é‡ŠæŠ¥å‘Š

## åˆ†ææ—¶é—´
{time.strftime('%Y-%m-%d %H:%M:%S')}

## æ•°æ®ä¿¡æ¯
- æ ·æœ¬æ•°é‡: {len(data)} ä¸ª
- åˆ†æå…ƒç´ : {', '.join(elements)}
- ç›®æ ‡çŸ¿ç§: {st.session_state.get('target_mineral', 'Au')}

## AIåœ°è´¨è§£é‡Š

{interpretation}

---
*æœ¬æŠ¥å‘Šç”±Gold-Seeker AIç³»ç»Ÿç”Ÿæˆï¼ŒåŸºäºDeepSeekå¤§æ¨¡å‹åˆ†æ*
"""
                    
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½æŠ¥å‘Š",
                        data=report_content,
                        file_name=f"geological_interpretation_{time.strftime('%Y%m%d_%H%M%S')}.md",
                        mime="text/markdown"
                    )

if __name__ == "__main__":
    # æŠ‘åˆ¶è­¦å‘Š
    warnings.filterwarnings('ignore')
    
    # è¿è¡Œåº”ç”¨
    main()