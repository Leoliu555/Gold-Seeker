"""
GeochemProcessor - åœ°çƒåŒ–å­¦æ•°æ®æ¸…æ´—ä¸å˜æ¢

åŸºäºCarranzaç†è®ºå®ç°åœ°çƒåŒ–å­¦æ•°æ®çš„é¢„å¤„ç†ï¼ŒåŒ…æ‹¬ï¼š
1. æ£€æµ‹é™æ•°æ®å¤„ç†
2. ä¸­å¿ƒå¯¹æ•°æ¯”å˜æ¢(CLR)
3. å¼‚å¸¸å€¼æ£€æµ‹ä¸å¤„ç†
4. æ•°æ®æ ‡å‡†åŒ–

æ ¸å¿ƒåŠŸèƒ½ï¼š
- impute_censored_data(): å¤„ç†ä½äºæ£€æµ‹é™æ•°æ®
- transform_clr(): ä¸­å¿ƒå¯¹æ•°æ¯”å˜æ¢
- detect_outliers(): å¼‚å¸¸å€¼æ£€æµ‹
- standardize_data(): æ•°æ®æ ‡å‡†åŒ–
"""

from typing import Dict, List, Any, Optional, Tuple, Union, Literal
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.patches import Circle
from scipy import stats
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.covariance import EllipticEnvelope
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings('ignore')

# Try to import pykrige for kriging interpolation
try:
    from pykrige.ok import OrdinaryKriging
    PYKRIGE_AVAILABLE = True
except ImportError:
    PYKRIGE_AVAILABLE = False
    print("Warning: pykrige not available. Kriging interpolation will use scipy instead.")

# Try to import scipy for interpolation
try:
    from scipy.interpolate import griddata
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False


class GeochemProcessor:
    """
    åœ°çƒåŒ–å­¦æ•°æ®å¤„ç†å™¨
    
    åŸºäºCarranza (2009) ç¬¬2ç« æ–¹æ³•ï¼Œå®ç°åœ°çƒåŒ–å­¦æ•°æ®çš„
    ä¸“ä¸šé¢„å¤„ç†ï¼Œä¸ºåç»­ç»Ÿè®¡åˆ†ææä¾›é«˜è´¨é‡æ•°æ®ã€‚
    
    å‚è€ƒæ–‡çŒ®ï¼š
    Carranza, E.J.M. (2009). Geochemical Anomaly and Mineral Prospectivity Mapping in GIS.
    """
    
    def __init__(self, detection_limits: Optional[Dict[str, float]] = None,
                 censoring_method: str = 'substitution'):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        
        Args:
            detection_limits: æ£€æµ‹é™å­—å…¸ {å…ƒç´ : æ£€æµ‹é™å€¼}
            censoring_method: æ£€æµ‹é™æ•°æ®å¤„ç†æ–¹æ³• ('substitution', 'ros', 'mle')
        """
        self.detection_limits = detection_limits or {}
        self.censoring_method = censoring_method
        self.scaler = None
        self.processing_log = []
        
    def impute_censored_data(self, df: pd.DataFrame,
                           elements: Optional[List[str]] = None,
                           method: Optional[Literal['substitution', 'ros', 'mle']] = None) -> pd.DataFrame:
        """
        å¤„ç†ä½äºæ£€æµ‹é™æ•°æ®
        
        æ ¹æ®Carranza (2009) 2.3èŠ‚æ–¹æ³•ï¼Œå¤„ç†åœ°çƒåŒ–å­¦æ•°æ®ä¸­
        å¸¸è§çš„æ£€æµ‹é™ä»¥ä¸‹å€¼ï¼ˆå·¦æˆªæ–­æ•°æ®ï¼‰ã€‚
        
        Args:
            df: åŸå§‹åœ°çƒåŒ–å­¦æ•°æ®
            elements: å¾…å¤„ç†å…ƒç´ åˆ—è¡¨
            method: å¤„ç†æ–¹æ³• ('substitution', 'ros', 'mle')
            
        Returns:
            pd.DataFrame: å¤„ç†åçš„æ•°æ®
            
        Methods:
        - substitution: æ›¿ä»£æ³•ï¼ˆæ£€æµ‹é™/2æˆ–æ£€æµ‹é™/âˆš2ï¼‰
        - ros: Regression on Order Statistics
        - mle: Maximum Likelihood Estimation
        
        Example:
            >>> processor = GeochemProcessor(
            ...     detection_limits={'Au': 0.1, 'As': 1.0, 'Sb': 0.5}
            ... )
            >>> processed_data = processor.impute_censored_data(
            ...     raw_data, 
            ...     elements=['Au', 'As', 'Sb'],
            ...     method='substitution'
            ... )
        """
        if method is None:
            method = self.censoring_method
            
        if elements is None:
            elements = [col for col in df.columns if df[col].dtype in ['float64', 'int64']]
        
        processed_df = df.copy()
        
        for element in elements:
            if element not in self.detection_limits:
                continue
                
            detection_limit = self.detection_limits[element]
            censored_mask = df[element] < detection_limit
            censored_count = censored_mask.sum()
            
            if censored_count == 0:
                continue
            
            # è®°å½•å¤„ç†ä¿¡æ¯
            self.processing_log.append({
                'element': element,
                'operation': 'censoring_imputation',
                'method': method,
                'censored_count': censored_count,
                'detection_limit': detection_limit
            })
            
            if method == 'substitution':
                # æ›¿ä»£æ³•ï¼šä½¿ç”¨æ£€æµ‹é™/2æˆ–æ£€æµ‹é™/âˆš2
                if censored_count / len(df) > 0.5:  # è¶…è¿‡50%æ•°æ®è¢«æˆªæ–­
                    substitution_value = detection_limit / np.sqrt(2)
                else:
                    substitution_value = detection_limit / 2
                    
                processed_df.loc[censored_mask, element] = substitution_value
                
            elif method == 'ros':
                # ROSæ–¹æ³•ï¼ˆç®€åŒ–å®ç°ï¼‰
                # æ£€æµ‹åˆ°çš„æ•°æ®
                detected_data = df[element][~censored_mask].dropna()
                if len(detected_data) > 0:
                    # å¯¹æ•°å˜æ¢
                    log_detected = np.log10(detected_data)
                    log_dl = np.log10(detection_limit)
                    
                    # çº¿æ€§å›å½’å¤–æ¨
                    rank = stats.rankdata(detected_data)
                    log_rank = np.log10(rank)
                    
                    if len(detected_data) > 2:
                        slope, intercept, r_value, p_value, std_err = stats.linregress(
                            log_rank, log_detected
                        )
                        
                        # ä¸ºæˆªæ–­æ•°æ®ç”Ÿæˆä¼°è®¡å€¼
                        censored_ranks = np.arange(1, censored_count + 1)
                        log_censored_estimates = slope * np.log10(censored_ranks) + intercept
                        censored_estimates = 10 ** log_censored_estimates
                        
                        # ç¡®ä¿ä¸è¶…è¿‡æ£€æµ‹é™
                        censored_estimates = np.minimum(censored_estimates, detection_limit * 0.99)
                        processed_df.loc[censored_mask, element] = censored_estimates
                    else:
                        # å›é€€åˆ°æ›¿ä»£æ³•
                        processed_df.loc[censored_mask, element] = detection_limit / 2
                        
            elif method == 'mle':
                # æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆç®€åŒ–å®ç°ï¼‰
                detected_data = df[element][~censored_mask].dropna()
                if len(detected_data) > 5:
                    # å‡è®¾å¯¹æ•°æ­£æ€åˆ†å¸ƒ
                    log_detected = np.log10(detected_data)
                    mu_hat = log_detected.mean()
                    sigma_hat = log_detected.std(ddof=1)
                    
                    # ä½¿ç”¨æˆªæ–­æ­£æ€åˆ†å¸ƒçš„æœŸæœ›å€¼
                    from scipy.stats import truncnorm
                    a = (np.log10(detection_limit) - mu_hat) / sigma_hat
                    truncated_mean = mu_hat - sigma_hat * (
                        stats.norm.pdf(a) / (1 - stats.norm.cdf(a))
                    )
                    
                    censored_estimates = 10 ** truncated_mean
                    processed_df.loc[censored_mask, element] = censored_estimates
                else:
                    # å›é€€åˆ°æ›¿ä»£æ³•
                    processed_df.loc[censored_mask, element] = detection_limit / 2
        
        return processed_df
    
    def transform_clr(self, df: pd.DataFrame,
                     elements: Optional[List[str]] = None,
                     add_small_constant: float = 1e-6) -> pd.DataFrame:
        """
        ä¸­å¿ƒå¯¹æ•°æ¯”å˜æ¢ (Centered Log-ratio Transformation)
        
        æ ¹æ®Aitchison (1986) ç»„æˆæ•°æ®åˆ†ææ–¹æ³•ï¼Œæ¶ˆé™¤åœ°çƒåŒ–å­¦
        æ•°æ®çš„é—­åˆæ•ˆåº”ï¼Œè¿™æ˜¯Carranza (2009) æ¨èçš„é¢„å¤„ç†æ­¥éª¤ã€‚
        
        Args:
            df: è¾“å…¥æ•°æ®
            elements: å˜æ¢å…ƒç´ åˆ—è¡¨
            add_small_constant: æ·»åŠ çš„å°å¸¸æ•°ï¼ˆé¿å…log(0)ï¼‰
            
        Returns:
            pd.DataFrame: CLRå˜æ¢åçš„æ•°æ®
            
        Mathematical Background:
        CLRå˜æ¢å…¬å¼: clr(x) = [ln(xâ‚/g(x)), ln(xâ‚‚/g(x)), ..., ln(x_D/g(x))]
        å…¶ä¸­ g(x) = (xâ‚ Ã— xâ‚‚ Ã— ... Ã— x_D)^(1/D) æ˜¯å‡ ä½•å¹³å‡
        
        Example:
            >>> clr_data = processor.transform_clr(
            ...     geochem_df,
            ...     elements=['Au', 'As', 'Sb', 'Cu', 'Pb', 'Zn']
            ... )
            >>> print(f"å˜æ¢åæ•°æ®å½¢çŠ¶: {clr_data.shape}")
        """
        if elements is None:
            elements = [col for col in df.columns if df[col].dtype in ['float64', 'int64']]
        
        # æå–æ•°æ®å¹¶æ·»åŠ å°å¸¸æ•°
        data = df[elements].copy()
        data = data + add_small_constant
        
        # æ£€æŸ¥è´Ÿå€¼
        if (data < 0).any().any():
            raise ValueError("CLRå˜æ¢è¦æ±‚æ•°æ®å¿…é¡»ä¸ºæ­£å€¼")
        
        # è®¡ç®—å‡ ä½•å¹³å‡
        geometric_mean = np.exp(np.log(data).mean(axis=1))
        
        # CLRå˜æ¢
        clr_data = np.log(data.div(geometric_mean, axis=0))
        
        # æ·»åŠ CLRå‰ç¼€åˆ°åˆ—å
        clr_columns = [f'CLR_{elem}' for elem in elements]
        clr_df = pd.DataFrame(clr_data, columns=clr_columns, index=df.index)
        
        # è®°å½•å¤„ç†ä¿¡æ¯
        self.processing_log.append({
            'operation': 'clr_transformation',
            'elements': elements,
            'shape_before': data.shape,
            'shape_after': clr_df.shape
        })
        
        return clr_df
    
    def detect_outliers(self, df: pd.DataFrame,
                        elements: Optional[List[str]] = None,
                        method: str = 'robust',
                        contamination: float = 0.05) -> Dict[str, Any]:
        """
        å¼‚å¸¸å€¼æ£€æµ‹
        
        ä½¿ç”¨å¤šç§æ–¹æ³•æ£€æµ‹åœ°çƒåŒ–å­¦æ•°æ®ä¸­çš„å¼‚å¸¸å€¼ï¼Œ
        åŒ…æ‹¬ç»Ÿè®¡æ–¹æ³•å’ŒåŸºäºåæ–¹å·®çš„æ–¹æ³•ã€‚
        
        Args:
            df: è¾“å…¥æ•°æ®
            elements: æ£€æµ‹å…ƒç´ åˆ—è¡¨
            method: æ£€æµ‹æ–¹æ³• ('zscore', 'iqr', 'robust', 'elliptic')
            contamination: å¼‚å¸¸å€¼æ¯”ä¾‹ä¼°è®¡
            
        Returns:
            Dict: åŒ…å«å¼‚å¸¸å€¼æ£€æµ‹ç»“æœå’Œå¯è§†åŒ–
            
        Example:
            >>> outliers = processor.detect_outliers(
            ...     geochem_df,
            ...     elements=['Au', 'As', 'Sb'],
            ...     method='robust'
            ... )
            >>> print(f"æ£€æµ‹åˆ° {len(outliers['outlier_indices'])} ä¸ªå¼‚å¸¸æ ·å“")
        """
        if elements is None:
            elements = [col for col in df.columns if df[col].dtype in ['float64', 'int64']]
        
        data = df[elements].copy()
        outlier_indices = set()
        outlier_scores = {}
        
        if method == 'zscore':
            # Z-scoreæ–¹æ³•
            z_scores = np.abs(stats.zscore(data, nan_policy='omit'))
            outlier_mask = z_scores > 3
            outlier_indices.update(data[outlier_mask.any(axis=1)].index)
            outlier_scores['zscore'] = z_scores.max(axis=1)
            
        elif method == 'iqr':
            # å››åˆ†ä½è·æ–¹æ³•
            Q1 = data.quantile(0.25)
            Q3 = data.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            outlier_mask = (data < lower_bound) | (data > upper_bound)
            outlier_indices.update(data[outlier_mask.any(axis=1)].index)
            
        elif method == 'robust':
            # åŸºäºé²æ£’ç»Ÿè®¡çš„æ–¹æ³•
            median = data.median()
            mad = np.abs(data - median).median()
            modified_z_scores = 0.6745 * (data - median) / mad
            outlier_mask = np.abs(modified_z_scores) > 3.5
            outlier_indices.update(data[outlier_mask.any(axis=1)].index)
            outlier_scores['robust_zscore'] = modified_z_scores.abs().max(axis=1)
            
        elif method == 'elliptic':
            # æ¤­åœ†åŒ…ç»œæ–¹æ³•
            detector = EllipticEnvelope(contamination=contamination, random_state=42)
            outlier_labels = detector.fit_predict(data.fillna(data.median()))
            outlier_mask = outlier_labels == -1
            outlier_indices.update(data[outlier_mask].index)
            outlier_scores['elliptic'] = detector.decision_function(data.fillna(data.median()))
        
        # å¯è§†åŒ–å¼‚å¸¸å€¼
        if len(elements) >= 2:
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle(f'Outlier Detection Results ({method.upper()} method)', 
                        fontsize=16, fontweight='bold')
            
            # å‰ä¸¤ä¸ªå…ƒç´ çš„æ•£ç‚¹å›¾
            elem1, elem2 = elements[0], elements[1]
            normal_data = data[~data.index.isin(outlier_indices)]
            outlier_data = data[data.index.isin(outlier_indices)]
            
            axes[0, 0].scatter(normal_data[elem1], normal_data[elem2], 
                             c='blue', label='Normal', alpha=0.6)
            axes[0, 0].scatter(outlier_data[elem1], outlier_data[elem2], 
                             c='red', label='Outliers', alpha=0.8)
            axes[0, 0].set_xlabel(elem1)
            axes[0, 0].set_ylabel(elem2)
            axes[0, 0].set_title(f'{elem1} vs {elem2}')
            axes[0, 0].legend()
            
            # ç®±çº¿å›¾
            data_melted = data.melt(var_name='Element', value_name='Concentration')
            outlier_indicator = data_melted.index.isin(list(outlier_indices) * len(elements))
            data_melted['Type'] = ['Outlier' if i in outlier_indices else 'Normal' 
                                  for i in data_melted.index // len(elements)]
            
            sns.boxplot(data=data_melted, x='Element', y='Concentration', 
                       hue='Type', ax=axes[0, 1])
            axes[0, 1].set_title('Boxplot by Element')
            axes[0, 1].tick_params(axis='x', rotation=45)
            
            # å¼‚å¸¸å€¼åˆ†æ•°åˆ†å¸ƒ
            if outlier_scores:
                score_name = list(outlier_scores.keys())[0]
                scores = outlier_scores[score_name]
                axes[1, 0].hist(scores, bins=30, alpha=0.7)
                axes[1, 0].axvline(x=np.percentile(scores, 95), color='r', 
                                 linestyle='--', label='95th percentile')
                axes[1, 0].set_xlabel(f'{score_name} Score')
                axes[1, 0].set_ylabel('Frequency')
                axes[1, 0].set_title('Outlier Score Distribution')
                axes[1, 0].legend()
            
            # å¼‚å¸¸å€¼ç»Ÿè®¡
            outlier_counts = data.index.isin(outlier_indices).groupby(data.index).sum()
            axes[1, 1].bar(range(len(outlier_counts)), outlier_counts.values)
            axes[1, 1].set_xlabel('Sample Index')
            axes[1, 1].set_ylabel('Number of Outlier Elements')
            axes[1, 1].set_title('Outlier Count per Sample')
            
            plt.tight_layout()
        else:
            fig = None
        
        return {
            'outlier_indices': list(outlier_indices),
            'outlier_scores': outlier_scores,
            'method': method,
            'contamination': contamination,
            'visualization': fig,
            'summary': {
                'total_samples': len(data),
                'outlier_samples': len(outlier_indices),
                'outlier_percentage': len(outlier_indices) / len(data) * 100
            }
        }
    
    def standardize_data(self, df: pd.DataFrame,
                        elements: Optional[List[str]] = None,
                        method: str = 'standard') -> Tuple[pd.DataFrame, Any]:
        """
        æ•°æ®æ ‡å‡†åŒ–
        
        Args:
            df: è¾“å…¥æ•°æ®
            elements: æ ‡å‡†åŒ–å…ƒç´ åˆ—è¡¨
            method: æ ‡å‡†åŒ–æ–¹æ³• ('standard', 'robust', 'minmax')
            
        Returns:
            Tuple[pd.DataFrame, scaler]: æ ‡å‡†åŒ–åçš„æ•°æ®å’Œæ ‡å‡†åŒ–å™¨
            
        Example:
            >>> scaled_data, scaler = processor.standardize_data(
            ...     geochem_df, method='robust'
            ... )
            >>> print(f"æ ‡å‡†åŒ–åå‡å€¼: {scaled_data.mean().mean():.6f}")
        """
        if elements is None:
            elements = [col for col in df.columns if df[col].dtype in ['float64', 'int64']]
        
        data = df[elements].copy()
        
        if method == 'standard':
            scaler = StandardScaler()
        elif method == 'robust':
            scaler = RobustScaler()
        elif method == 'minmax':
            from sklearn.preprocessing import MinMaxScaler
            scaler = MinMaxScaler()
        else:
            raise ValueError(f"Unknown scaling method: {method}")
        
        # å¤„ç†ç¼ºå¤±å€¼
        data_filled = data.fillna(data.median())
        
        # æ ‡å‡†åŒ–
        scaled_data = scaler.fit_transform(data_filled)
        scaled_df = pd.DataFrame(scaled_data, columns=elements, index=df.index)
        
        self.scaler = scaler
        
        # è®°å½•å¤„ç†ä¿¡æ¯
        self.processing_log.append({
            'operation': 'standardization',
            'method': method,
            'elements': elements,
            'scaler_params': scaler.get_params() if hasattr(scaler, 'get_params') else None
        })
        
        return scaled_df, scaler
    
    def get_processing_summary(self) -> pd.DataFrame:
        """
        è·å–æ•°æ®å¤„ç†æ‘˜è¦
        
        Returns:
            pd.DataFrame: å¤„ç†æ­¥éª¤æ‘˜è¦
        """
        if not self.processing_log:
            return pd.DataFrame()
        
        return pd.DataFrame(self.processing_log)
    
    def plot_data_distribution(self, df: pd.DataFrame,
                              elements: Optional[List[str]] = None,
                              plot_type: str = 'histogram',
                              figsize: Tuple[int, int] = (15, 10)) -> plt.Figure:
        """
        ç»˜åˆ¶æ•°æ®åˆ†å¸ƒå›¾
        
        Args:
            df: è¾“å…¥æ•°æ®
            elements: ç»˜å›¾å…ƒç´ åˆ—è¡¨
            plot_type: å›¾è¡¨ç±»å‹ ('histogram', 'boxplot', 'violin', 'qq')
            figsize: å›¾å½¢å°ºå¯¸
            
        Returns:
            plt.Figure: å›¾å½¢å¯¹è±¡
        """
        if elements is None:
            elements = [col for col in df.columns if df[col].dtype in ['float64', 'int64']]
        
        data = df[elements].copy()
        n_elements = len(elements)
        
        # è®¡ç®—å­å›¾å¸ƒå±€
        cols = min(4, n_elements)
        rows = (n_elements + cols - 1) // cols
        
        fig, axes = plt.subplots(rows, cols, figsize=figsize)
        if n_elements == 1:
            axes = [axes]
        elif rows == 1:
            axes = axes.reshape(1, -1)
        
        fig.suptitle(f'Data Distribution ({plot_type.title()})', 
                    fontsize=16, fontweight='bold')
        
        for i, element in enumerate(elements):
            row, col = i // cols, i % cols
            ax = axes[row, col] if rows > 1 else axes[col]
            
            element_data = data[element].dropna()
            
            if plot_type == 'histogram':
                ax.hist(element_data, bins=30, alpha=0.7, edgecolor='black')
                ax.set_xlabel('Value')
                ax.set_ylabel('Frequency')
                
            elif plot_type == 'boxplot':
                ax.boxplot(element_data)
                ax.set_ylabel('Value')
                
            elif plot_type == 'violin':
                sns.violinplot(y=element_data, ax=ax)
                
            elif plot_type == 'qq':
                stats.probplot(element_data, dist="norm", plot=ax)
                
            ax.set_title(f'{element}')
            
        # éšè—å¤šä½™çš„å­å›¾
        for i in range(n_elements, rows * cols):
            row, col = i // cols, i % cols
            ax = axes[row, col] if rows > 1 else axes[col]
            ax.set_visible(False)
        
        plt.tight_layout()
        return fig
    
    def get_correlation_matrix(self, df: pd.DataFrame, 
                               elements: Optional[List[str]] = None) -> pd.DataFrame:
        """
        è®¡ç®—å…ƒç´ ç›¸å…³æ€§çŸ©é˜µ
        
        Args:
            df: è¾“å…¥æ•°æ®
            elements: åˆ†æå…ƒç´ åˆ—è¡¨
            
        Returns:
            pd.DataFrame: ç›¸å…³æ€§çŸ©é˜µ
        """
        if elements is None:
            elements = [col for col in df.columns if df[col].dtype in ['float64', 'int64']]
        
        return df[elements].corr()
    
    def plot_correlation_heatmap(self, df: pd.DataFrame, 
                                 elements: Optional[List[str]] = None,
                                 figsize: Tuple[int, int] = (10, 8)) -> plt.Figure:
        """
        ç»˜åˆ¶ç›¸å…³æ€§çƒ­åŠ›å›¾ (è‹±æ–‡æ ‡ç­¾)
        
        Args:
            df: è¾“å…¥æ•°æ®
            elements: åˆ†æå…ƒç´ åˆ—è¡¨
            figsize: å›¾å½¢å°ºå¯¸
            
        Returns:
            plt.Figure: å›¾å½¢å¯¹è±¡
        """
        if elements is None:
            elements = [col for col in df.columns if df[col].dtype in ['float64', 'int64']]
        
        corr_matrix = self.get_correlation_matrix(df, elements)
        
        fig, ax = plt.subplots(figsize=figsize)
        sns.heatmap(corr_matrix, annot=True, cmap='RdYlBu_r', center=0, 
                    square=True, ax=ax, cbar_kws={'label': 'Correlation Coefficient'})
        ax.set_title('Element Correlation Heatmap', fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        return fig
    
    def plot_pca_loadings(self, df: pd.DataFrame, 
                          elements: Optional[List[str]] = None,
                          figsize: Tuple[int, int] = (10, 8)) -> plt.Figure:
        """
        ç»˜åˆ¶PCAè½½è·å›¾ (è‹±æ–‡æ ‡ç­¾)
        
        Args:
            df: è¾“å…¥æ•°æ®
            elements: åˆ†æå…ƒç´ åˆ—è¡¨
            figsize: å›¾å½¢å°ºå¯¸
            
        Returns:
            plt.Figure: å›¾å½¢å¯¹è±¡
        """
        if elements is None:
            elements = [col for col in df.columns if df[col].dtype in ['float64', 'int64']]
        
        # æ ‡å‡†åŒ–æ•°æ®
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(df[elements])
        
        # PCAåˆ†æ
        pca = PCA(n_components=2)
        pca.fit(scaled_data)
        
        # åˆ›å»ºè½½è·å›¾
        fig, ax = plt.subplots(figsize=figsize)
        
        # ç»˜åˆ¶è½½è·å‘é‡
        for i, element in enumerate(elements):
            ax.arrow(0, 0, pca.components_[0, i], pca.components_[1, i],
                    head_width=0.05, head_length=0.05, fc='red', ec='red')
            ax.text(pca.components_[0, i]*1.1, pca.components_[1, i]*1.1, 
                    element, fontsize=12, ha='center', va='center')
        
        # æ·»åŠ å‚è€ƒåœ†
        circle = Circle((0, 0), 1, fill=False, color='blue', linestyle='--')
        ax.add_patch(circle)
        
        ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} Variance)', fontsize=12)
        ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} Variance)', fontsize=12)
        ax.set_title('PCA Loading Plot', fontsize=16, fontweight='bold')
        ax.grid(True, alpha=0.3)
        ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)
        ax.axvline(x=0, color='k', linestyle='-', alpha=0.3)
        plt.tight_layout()
        
        return fig
    
    def plot_dendrogram(self, df: pd.DataFrame, 
                       elements: Optional[List[str]] = None,
                       figsize: Tuple[int, int] = (12, 8)) -> plt.Figure:
        """
        ç»˜åˆ¶Rå‹èšç±»æ ‘çŠ¶å›¾ (è‹±æ–‡æ ‡ç­¾)
        
        Args:
            df: è¾“å…¥æ•°æ®
            elements: åˆ†æå…ƒç´ åˆ—è¡¨
            figsize: å›¾å½¢å°ºå¯¸
            
        Returns:
            plt.Figure: å›¾å½¢å¯¹è±¡
        """
        from scipy.cluster.hierarchy import linkage, dendrogram
        from scipy.spatial.distance import pdist
        
        if elements is None:
            elements = [col for col in df.columns if df[col].dtype in ['float64', 'int64']]
        
        # è®¡ç®—ç›¸å…³æ€§è·ç¦»
        corr_matrix = df[elements].corr()
        distance_matrix = 1 - np.abs(corr_matrix)
        condensed_distances = pdist(distance_matrix.values)
        
        # å±‚æ¬¡èšç±»
        linkage_matrix = linkage(condensed_distances, method='ward')
        
        fig, ax = plt.subplots(figsize=figsize)
        dendrogram(linkage_matrix, labels=elements, ax=ax, 
                   leaf_rotation=45, leaf_font_size=12)
        ax.set_title('R-mode Cluster Dendrogram', fontsize=16, fontweight='bold')
        ax.set_xlabel('Elements', fontsize=12)
        ax.set_ylabel('Distance', fontsize=12)
        plt.tight_layout()
        
        return fig
    
    def interpolate_kriging(self, df: pd.DataFrame, 
                           target_element: str = 'Au',
                           x_col: str = 'X', y_col: str = 'Y',
                           grid_resolution: float = 0.01,
                           variogram_model: Literal['spherical', 'exponential', 'gaussian'] = 'spherical') -> Dict[str, Any]:
        """
        ä¸“ä¸šå…‹é‡Œé‡‘æ’å€¼ - åŒ…å«å˜å¼‚å‡½æ•°åˆ†æ
        
        Args:
            df: åŒ…å«åæ ‡å’Œå…ƒç´ æµ“åº¦çš„æ•°æ®æ¡†
            target_element: ç›®æ ‡å…ƒç´ åˆ—å
            x_col: Xåæ ‡åˆ—å (ç»åº¦)
            y_col: Yåæ ‡åˆ—å (çº¬åº¦)
            grid_resolution: ç½‘æ ¼åˆ†è¾¨ç‡ (åº¦)
            variogram_model: å˜å¼‚å‡½æ•°æ¨¡å‹ ('spherical', 'exponential')
            
        Returns:
            Dict: åŒ…å«æ’å€¼ç»“æœçš„å­—å…¸
                {
                    'grid_x': ç½‘æ ¼Xåæ ‡,
                    'grid_y': ç½‘æ ¼Yåæ ‡,
                    'grid_z': æ’å€¼ç»“æœçŸ©é˜µ,
                    'extent': [xmin, xmax, ymin, ymax],
                    'variogram_params': {'nugget': nugget, 'sill': sill, 'range': range_val},
                    'figure': matplotlib Figureå¯¹è±¡
                }
        """
        # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
        required_cols = [x_col, y_col, target_element]
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"Missing required columns: {missing_cols}")
        
        # æ¸…ç†æ•°æ®ï¼ˆç§»é™¤NaNå€¼ï¼‰
        clean_data = df[required_cols].dropna()
        if len(clean_data) < 10:
            raise ValueError(f"Insufficient valid data points for kriging: {len(clean_data)} (minimum 10 required)")
        
        x = clean_data[x_col].values
        y = clean_data[y_col].values
        z = clean_data[target_element].values
        
        # å¯¹æ•°å˜æ¢ï¼ˆåœ°çƒåŒ–å­¦æ•°æ®é€šå¸¸å‘ˆå¯¹æ•°æ­£æ€åˆ†å¸ƒï¼‰
        log_z = np.log10(z + 1e-6)  # é¿å…log(0)
        
        # åˆ›å»ºç½‘æ ¼
        xmin, xmax = x.min(), x.max()
        ymin, ymax = y.min(), y.max()
        
        # æ‰©å±•è¾¹ç•Œ
        x_range = xmax - xmin
        y_range = ymax - ymin
        xmin -= x_range * 0.1
        xmax += x_range * 0.1
        ymin -= y_range * 0.1
        ymax += y_range * 0.1
        
        # è®¡ç®—ç½‘æ ¼ç‚¹æ•°
        nx = int((xmax - xmin) / grid_resolution) + 1
        ny = int((ymax - ymin) / grid_resolution) + 1
        
        # å†…å­˜ä¼˜åŒ–ï¼šé™åˆ¶ç½‘æ ¼å¤§å°
        max_grid_size = 500  # æœ€å¤§ç½‘æ ¼å°ºå¯¸
        if nx > max_grid_size or ny > max_grid_size:
            # è‡ªåŠ¨è°ƒæ•´åˆ†è¾¨ç‡
            scale_factor = max(nx / max_grid_size, ny / max_grid_size)
            new_resolution = grid_resolution * scale_factor
            nx = int((xmax - xmin) / new_resolution) + 1
            ny = int((ymax - ymin) / new_resolution) + 1
            
            print(f"âš ï¸ Grid too large ({nx}Ã—{ny}), adjusting resolution to {new_resolution:.6f}")
            print(f"   Original resolution: {grid_resolution}")
            print(f"   Adjusted grid size: {nx}Ã—{ny}")
            grid_resolution = new_resolution
        
        # æ£€æŸ¥å†…å­˜éœ€æ±‚
        memory_mb = (nx * ny * 8 * 3) / (1024 * 1024)  # 3ä¸ªfloat64æ•°ç»„
        if memory_mb > 1000:  # è¶…è¿‡1GBå†…å­˜
            raise MemoryError(f"Grid too large: {nx}Ã—{ny} requires {memory_mb:.1f}MB memory. " +
                            f"Please increase resolution or reduce data extent.")
        
        grid_x = np.linspace(xmin, xmax, nx)
        grid_y = np.linspace(ymin, ymax, ny)
        
        print(f"ğŸ” Kriging Analysis for {target_element}:")
        print(f"   - Data points: {len(x)}")
        print(f"   - Grid resolution: {grid_resolution}Â°")
        print(f"   - Grid size: {nx} x {ny}")
        print(f"   - Estimated memory usage: {memory_mb:.1f}MB")
        
        # æ‰§è¡Œå…‹é‡Œé‡‘æ’å€¼
        if PYKRIGE_AVAILABLE:
            try:
                # ä½¿ç”¨pykrigeè¿›è¡Œæ™®é€šå…‹é‡Œé‡‘æ’å€¼
                OK = OrdinaryKriging(
                    x, y, log_z,
                    variogram_model=variogram_model,
                    verbose=True,  # æ˜¾ç¤ºå˜å¼‚å‡½æ•°å‚æ•°
                    enable_plotting=False,
                    coordinates_type='geographic'
                )
                
                grid_z, _ = OK.execute('grid', grid_x, grid_y)
                
                # åå¯¹æ•°å˜æ¢
                grid_z = 10 ** grid_z - 1e-6
                
                # è·å–å˜å¼‚å‡½æ•°å‚æ•°
                if hasattr(OK, 'variogram_parameters'):
                    variogram_params = OK.variogram_parameters
                else:
                    # ä¼°ç®—å˜å¼‚å‡½æ•°å‚æ•°
                    variogram_params = self._estimate_variogram_params(x, y, log_z)
                
                print(f"   - Variogram model: {variogram_model}")
                print(f"   - Nugget: {variogram_params.get('nugget', 'N/A')}")
                print(f"   - Sill: {variogram_params.get('sill', 'N/A')}")
                print(f"   - Range: {variogram_params.get('range', 'N/A')}")
                
            except Exception as e:
                print(f"âš ï¸ PyKrige failed: {e}")
                print("ğŸ”„ Using scipy interpolation as fallback...")
                pykrige_failed = True
            else:
                pykrige_failed = False
        
        if not PYKRIGE_AVAILABLE or pykrige_failed:
            # ä½¿ç”¨scipyè¿›è¡Œæ’å€¼ï¼ˆå›é€€æ–¹æ¡ˆï¼‰
            if not SCIPY_AVAILABLE:
                raise ImportError("Neither pykrige nor scipy is available for interpolation")
            
            grid_xx, grid_yy = np.meshgrid(grid_x, grid_y)
            points = np.column_stack((x, y))
            grid_z = griddata(points, z, (grid_xx, grid_yy), method='cubic')
            
            variogram_params = {'nugget': 0, 'sill': np.var(z), 'range': x_range/2}
        
        # å¤„ç†æ’å€¼ç»“æœä¸­çš„NaNå€¼
        if np.any(np.isnan(grid_z)):
            from scipy.interpolate import NearestNDInterpolator
            interp = NearestNDInterpolator(np.column_stack((x, y)), z)
            grid_xx, grid_yy = np.meshgrid(grid_x, grid_y)
            nan_mask = np.isnan(grid_z)
            grid_z[nan_mask] = interp(grid_xx[nan_mask], grid_yy[nan_mask])
        
        # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
        fig = self._plot_kriging_result(grid_x, grid_y, grid_z, x, y, z, target_element, 
                                      [xmin, xmax, ymin, ymax], variogram_params)
        
        return {
            'grid_x': grid_x,
            'grid_y': grid_y,
            'grid_z': grid_z,
            'extent': [xmin, xmax, ymin, ymax],
            'variogram_params': variogram_params,
            'figure': fig
        }
    
    def _estimate_variogram_params(self, x: np.ndarray, y: np.ndarray, z: np.ndarray) -> Dict[str, float]:
        """ä¼°ç®—å˜å¼‚å‡½æ•°å‚æ•°"""
        # è®¡ç®—ç»éªŒå˜å¼‚å‡½æ•°
        distances = []
        semivariances = []
        
        n_points = len(x)
        for i in range(n_points):
            for j in range(i+1, n_points):
                dist = np.sqrt((x[i] - x[j])**2 + (y[i] - y[j])**2)
                semivar = 0.5 * (z[i] - z[j])**2
                distances.append(dist)
                semivariances.append(semivar)
        
        distances = np.array(distances)
        semivariances = np.array(semivariances)
        
        # ç®€å•ä¼°ç®—
        nugget = np.min(semivariances)
        sill = np.var(z)
        range_val = np.percentile(distances, 75)  # ä½¿ç”¨75%åˆ†ä½æ•°ä½œä¸ºå˜ç¨‹
        
        return {'nugget': nugget, 'sill': sill, 'range': range_val}
    
    def _plot_kriging_result(self, grid_x: np.ndarray, grid_y: np.ndarray, grid_z: np.ndarray,
                           x: np.ndarray, y: np.ndarray, z: np.ndarray, element: str,
                           extent: List[float], variogram_params: Dict[str, float]) -> plt.Figure:
        """ç»˜åˆ¶å…‹é‡Œé‡‘æ’å€¼ç»“æœ"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        # çƒ­åŠ›å›¾
        im = ax1.contourf(grid_x, grid_y, grid_z, levels=15, cmap='YlOrRd')
        ax1.scatter(x, y, c=z, s=30, edgecolors='black', linewidth=0.5, cmap='YlOrRd')
        ax1.set_xlabel('Longitude')
        ax1.set_ylabel('Latitude')
        ax1.set_title(f'Kriging Interpolation ({element})')
        plt.colorbar(im, ax=ax1, label=f'{element} Concentration')
        
        # ç­‰å€¼çº¿å›¾
        contour = ax2.contour(grid_x, grid_y, grid_z, levels=10, colors='black', linewidths=0.5)
        ax2.clabel(contour, inline=True, fontsize=8)
        ax2.scatter(x, y, c='red', s=20, alpha=0.7)
        ax2.set_xlabel('Longitude')
        ax2.set_ylabel('Latitude')
        ax2.set_title(f'Contour Map ({element})')
        
        # æ·»åŠ å˜å¼‚å‡½æ•°å‚æ•°ä¿¡æ¯
        param_text = f"Nugget: {variogram_params.get('nugget', 'N/A'):.3f}\n"
        param_text += f"Sill: {variogram_params.get('sill', 'N/A'):.3f}\n"
        param_text += f"Range: {variogram_params.get('range', 'N/A'):.3f}"
        ax2.text(0.02, 0.98, param_text, transform=ax2.transAxes, 
                 verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        plt.tight_layout()
        return fig
    
    def run_fractal_ca_model(self, df: pd.DataFrame,
                           target_element: str = 'Au',
                           x_col: str = 'X', y_col: str = 'Y',
                           grid_resolution: float = 0.01) -> Dict[str, Any]:
        """
        C-Aåˆ†å½¢åˆ†æ - åŸºäºæ …æ ¼çš„æµ“åº¦-é¢ç§¯åˆ†å½¢æ¨¡å‹
        
        Args:
            df: åŒ…å«åæ ‡å’Œå…ƒç´ æµ“åº¦çš„æ•°æ®æ¡†
            target_element: ç›®æ ‡å…ƒç´ åˆ—å
            x_col: Xåæ ‡åˆ—å (ç»åº¦)
            y_col: Yåæ ‡åˆ—å (çº¬åº¦)
            grid_resolution: æ …æ ¼åˆ†è¾¨ç‡ (åº¦)
            
        Returns:
            Dict: åŒ…å«C-Aåˆ†æç»“æœçš„å­—å…¸
                {
                    'threshold_value': å¼‚å¸¸é˜ˆå€¼,
                    'grid_x': æ …æ ¼Xåæ ‡,
                    'grid_y': æ …æ ¼Yåæ ‡,
                    'grid_z': æ …æ ¼åŒ–æµ“åº¦å€¼,
                    'log_area': ç´¯è®¡é¢ç§¯å¯¹æ•°,
                    'log_concentration': æµ“åº¦å¯¹æ•°,
                    'breakpoints': æ‹ç‚¹ä½ç½®,
                    'figure': matplotlib Figureå¯¹è±¡
                }
        """
        # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
        required_cols = [x_col, y_col, target_element]
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"Missing required columns: {missing_cols}")
        
        # æ¸…ç†æ•°æ®
        clean_data = df[required_cols].dropna()
        if len(clean_data) < 10:
            raise ValueError(f"Insufficient valid data points: {len(clean_data)} (minimum 10 required)")
        
        x = clean_data[x_col].values
        y = clean_data[y_col].values
        z = clean_data[target_element].values
        
        print(f"ğŸ” C-A Fractal Analysis for {target_element}:")
        print(f"   - Data points: {len(x)}")
        print(f"   - Grid resolution: {grid_resolution}Â°")
        
        # æ£€æŸ¥æ•°æ®èŒƒå›´å’Œåˆ†è¾¨ç‡
        x_range = x.max() - x.min()
        y_range = y.max() - y.min()
        estimated_grid_points = (x_range / grid_resolution) * (y_range / grid_resolution)
        
        if estimated_grid_points > 100000:  # è¶…è¿‡10ä¸‡ä¸ªç‚¹
            print(f"âš ï¸ Large dataset detected: ~{estimated_grid_points:.0f} grid points")
            print(f"   - Data extent: {x_range:.3f}Â° Ã— {y_range:.3f}Â°")
            print(f"   - Consider increasing resolution to reduce memory usage")
        
        # Step 1: æ …æ ¼åŒ– (Rasterization using IDW)
        grid_x, grid_y, grid_z = self._rasterize_data(x, y, z, grid_resolution)
        
        # Step 2: C-Aè®¡ç®—
        log_area, log_concentration = self._calculate_ca_relationship(grid_z)
        
        # Step 3: è‡ªåŠ¨åˆ†å‰² (å¯»æ‰¾æ‹ç‚¹)
        breakpoints, threshold_value = self._find_ca_breakpoints(log_area, log_concentration, grid_z)
        
        print(f"   - Anomaly threshold: {threshold_value:.3f}")
        print(f"   - Breakpoints found: {len(breakpoints)}")
        
        # Step 4: ç»˜å›¾
        fig = self._plot_ca_fractal(log_area, log_concentration, breakpoints, target_element)
        
        return {
            'threshold_value': threshold_value,
            'grid_x': grid_x,
            'grid_y': grid_y,
            'grid_z': grid_z,
            'log_area': log_area,
            'log_concentration': log_concentration,
            'breakpoints': breakpoints,
            'figure': fig
        }
    
    def _rasterize_data(self, x: np.ndarray, y: np.ndarray, z: np.ndarray, 
                       resolution: float) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """ä½¿ç”¨IDWå°†ç¦»æ•£ç‚¹æ•°æ®æ …æ ¼åŒ– (å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬)"""
        # åˆ›å»ºç½‘æ ¼
        xmin, xmax = x.min(), x.max()
        ymin, ymax = y.min(), y.max()
        
        # æ‰©å±•è¾¹ç•Œ
        x_range = xmax - xmin
        y_range = ymax - ymin
        xmin -= x_range * 0.1
        xmax += x_range * 0.1
        ymin -= y_range * 0.1
        ymax += y_range * 0.1
        
        nx = int((xmax - xmin) / resolution) + 1
        ny = int((ymax - ymin) / resolution) + 1
        
        # å†…å­˜ä¼˜åŒ–ï¼šé™åˆ¶ç½‘æ ¼å¤§å°
        max_grid_size = 500  # æœ€å¤§ç½‘æ ¼å°ºå¯¸
        if nx > max_grid_size or ny > max_grid_size:
            # è‡ªåŠ¨è°ƒæ•´åˆ†è¾¨ç‡
            scale_factor = max(nx / max_grid_size, ny / max_grid_size)
            new_resolution = resolution * scale_factor
            nx = int((xmax - xmin) / new_resolution) + 1
            ny = int((ymax - ymin) / new_resolution) + 1
            
            print(f"âš ï¸ Grid too large ({nx}Ã—{ny}), adjusting resolution to {new_resolution:.6f}")
            print(f"   Original resolution: {resolution}")
            print(f"   Adjusted grid size: {nx}Ã—{ny}")
        
        # æ£€æŸ¥å†…å­˜éœ€æ±‚
        memory_mb = (nx * ny * 8 * 3) / (1024 * 1024)  # 3ä¸ªfloat64æ•°ç»„
        if memory_mb > 1000:  # è¶…è¿‡1GBå†…å­˜
            raise MemoryError(f"Grid too large: {nx}Ã—{ny} requires {memory_mb:.1f}MB memory. " +
                            f"Please increase resolution or reduce data extent.")
        
        grid_x = np.linspace(xmin, xmax, nx)
        grid_y = np.linspace(ymin, ymax, ny)
        grid_xx, grid_yy = np.meshgrid(grid_x, grid_y)
        
        # IDWæ’å€¼ (ä¼˜åŒ–ç‰ˆæœ¬)
        grid_z = np.zeros_like(grid_xx)
        
        # ä½¿ç”¨å‘é‡åŒ–æ“ä½œä¼˜åŒ–IDWæ’å€¼
        for i in range(ny):
            for j in range(nx):
                point_x, point_y = grid_xx[i, j], grid_yy[i, j]
                
                # è®¡ç®—è·ç¦» (å‘é‡åŒ–)
                distances = np.sqrt((x - point_x)**2 + (y - point_y)**2)
                
                # æ‰¾åˆ°æœ€è¿‘çš„kä¸ªç‚¹ (ä¼˜åŒ–è®¡ç®—)
                k = min(12, len(x))  # ä½¿ç”¨æœ€è¿‘çš„12ä¸ªç‚¹
                nearest_indices = np.argpartition(distances, k)[:k]
                
                # è®¡ç®—æƒé‡
                nearest_distances = distances[nearest_indices]
                nearest_distances[nearest_distances == 0] = 1e-10
                weights = 1.0 / nearest_distances**2
                weights /= weights.sum()
                
                # åŠ æƒå¹³å‡
                grid_z[i, j] = np.sum(weights * z[nearest_indices])
        
        return grid_x, grid_y, grid_z
    
    def _calculate_ca_relationship(self, grid_z: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """è®¡ç®—C-Aå…³ç³» (Concentration-Area)"""
        # å°†2Dç½‘æ ¼å±•å¹³ä¸º1Dæ•°ç»„
        z_flat = grid_z.flatten()
        
        # ç§»é™¤NaNå€¼
        z_valid = z_flat[~np.isnan(z_flat)]
        
        # æŒ‰æµ“åº¦é™åºæ’åˆ—
        z_sorted = np.sort(z_valid)[::-1]
        
        # è®¡ç®—ç´¯è®¡é¢ç§¯ (åƒç´ æ•°)
        n_pixels = len(z_sorted)
        areas = np.arange(1, n_pixels + 1)
        
        # å–å¯¹æ•° (é¿å…log(0))
        concentration_positive = z_sorted[z_sorted > 0]
        areas_positive = areas[:len(concentration_positive)]
        
        log_concentration = np.log10(concentration_positive)
        log_area = np.log10(areas_positive)
        
        return log_area, log_concentration
    
    def _find_ca_breakpoints(self, log_area: np.ndarray, log_concentration: np.ndarray,
                           grid_z: np.ndarray) -> Tuple[List[int], float]:
        """ä½¿ç”¨åˆ†æ®µçº¿æ€§å›å½’å¯»æ‰¾C-Aæ›²çº¿æ‹ç‚¹"""
        try:
            # å°è¯•ä½¿ç”¨pwlfåº“è¿›è¡Œåˆ†æ®µçº¿æ€§å›å½’
            import pwlf
            
            # åˆ›å»ºåˆ†æ®µçº¿æ€§å›å½’æ¨¡å‹
            pwlf_model = pwlf.PiecewiseLinFit(log_area, log_concentration)
            
            # å°è¯•1-2ä¸ªæ‹ç‚¹
            min_bic = float('inf')
            best_breakpoints = []
            
            for n_breaks in range(1, 3):
                try:
                    breaks = pwlf_model.fit(n_breaks)
                    bic = pwlf_model.bic
                    
                    if bic < min_bic:
                        min_bic = bic
                        best_breakpoints = breaks
                except:
                    continue
            
            if len(best_breakpoints) > 0:
                # è·å–é˜ˆå€¼ (ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‹ç‚¹å¯¹åº”çš„æµ“åº¦å€¼)
                breakpoint_idx = np.argmin(np.abs(log_area - best_breakpoints[0]))
                threshold_idx = int(len(grid_z.flatten()) * (1 - breakpoint_idx / len(log_area)))
                threshold_value = np.sort(grid_z.flatten())[::-1][threshold_idx]
                
                return best_breakpoints, threshold_value
        
        except ImportError:
            print("   - pwlf not available, using simple breakpoint detection")
        except Exception as e:
            print(f"   - pwlf failed: {e}, using simple detection")
        
        # å›é€€åˆ°ç®€å•çš„åŸºäºæ®‹å·®çš„æ‹ç‚¹æ£€æµ‹
        return self._simple_breakpoint_detection(log_area, log_concentration, grid_z)
    
    def _simple_breakpoint_detection(self, log_area: np.ndarray, log_concentration: np.ndarray,
                                  grid_z: np.ndarray) -> Tuple[List[int], float]:
        """ç®€å•çš„æ‹ç‚¹æ£€æµ‹æ–¹æ³•"""
        n_points = len(log_area)
        
        # è®¡ç®—æ‰€æœ‰å¯èƒ½çš„æ‹ç‚¹ä½ç½®
        min_residual = float('inf')
        best_breakpoint = n_points // 3  # é»˜è®¤ä½ç½®
        
        # æœç´¢æœ€ä½³æ‹ç‚¹ä½ç½® (å‰1/3åˆ°2/3èŒƒå›´)
        for i in range(n_points // 3, 2 * n_points // 3):
            # åˆ†æ®µçº¿æ€§æ‹Ÿåˆ
            # ç¬¬ä¸€æ®µ
            x1, y1 = log_area[:i+1], log_concentration[:i+1]
            fit1 = np.polyfit(x1, y1, 1)
            pred1 = np.polyval(fit1, x1)
            residual1 = np.sum((y1 - pred1)**2)
            
            # ç¬¬äºŒæ®µ
            x2, y2 = log_area[i:], log_concentration[i:]
            fit2 = np.polyfit(x2, y2, 1)
            pred2 = np.polyval(fit2, x2)
            residual2 = np.sum((y2 - pred2)**2)
            
            total_residual = residual1 + residual2
            
            if total_residual < min_residual:
                min_residual = total_residual
                best_breakpoint = i
        
        # è®¡ç®—é˜ˆå€¼
        threshold_idx = int(len(grid_z.flatten()) * (1 - best_breakpoint / n_points))
        threshold_value = np.sort(grid_z.flatten())[::-1][threshold_idx]
        
        return [log_area[best_breakpoint]], threshold_value
    
    def _plot_ca_fractal(self, log_area: np.ndarray, log_concentration: np.ndarray,
                        breakpoints: List[float], element: str) -> plt.Figure:
        """ç»˜åˆ¶C-Aåˆ†å½¢åˆ†æå›¾"""
        fig, ax = plt.subplots(figsize=(10, 8))
        
        # ç»˜åˆ¶æ•£ç‚¹
        ax.scatter(log_concentration, log_area, alpha=0.6, s=20, c='blue', label='Data points')
        
        # åˆ†æ®µçº¿æ€§æ‹Ÿåˆ
        if len(breakpoints) > 0:
            # æ·»åŠ æ‹ç‚¹åˆ°æ•°æ®ä¸­
            all_x = np.concatenate([[log_concentration[0]], log_concentration, [log_concentration[-1]]])
            all_y = np.concatenate([[log_area[0]], log_area, [log_area[-1]]])
            
            # ä¸ºæ¯ä¸ªçº¿æ®µæ‹Ÿåˆ
            segments_x = []
            segments_y = []
            
            prev_idx = 0
            for bp in breakpoints:
                # æ‰¾åˆ°æœ€æ¥è¿‘æ‹ç‚¹çš„ç´¢å¼•
                bp_idx = np.argmin(np.abs(log_area - bp))
                
                # æ‹Ÿåˆçº¿æ®µ
                segment_x = log_concentration[prev_idx:bp_idx+1]
                segment_y = log_area[prev_idx:bp_idx+1]
                
                if len(segment_x) > 1:
                    fit = np.polyfit(segment_x, segment_y, 1)
                    x_fit = np.linspace(segment_x[0], segment_x[-1], 50)
                    y_fit = np.polyval(fit, x_fit)
                    
                    ax.plot(x_fit, y_fit, 'r--', linewidth=2, alpha=0.8)
                    segments_x.extend(x_fit)
                    segments_y.extend(y_fit)
                
                prev_idx = bp_idx
            
            # æœ€åä¸€æ®µ
            if prev_idx < len(log_concentration) - 1:
                segment_x = log_concentration[prev_idx:]
                segment_y = log_area[prev_idx:]
                
                if len(segment_x) > 1:
                    fit = np.polyfit(segment_x, segment_y, 1)
                    x_fit = np.linspace(segment_x[0], segment_x[-1], 50)
                    y_fit = np.polyval(fit, x_fit)
                    
                    ax.plot(x_fit, y_fit, 'r--', linewidth=2, alpha=0.8)
            
            # æ ‡è®°æ‹ç‚¹
            for bp in breakpoints:
                bp_idx = np.argmin(np.abs(log_area - bp))
                ax.plot(log_concentration[bp_idx], log_area[bp_idx], 'ro', 
                       markersize=8, label=f'Breakpoint')
        
        ax.set_xlabel('Log(Concentration)')
        ax.set_ylabel('Log(Area)')
        ax.set_title(f'C-A Fractal Analysis ({element})')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        return fig